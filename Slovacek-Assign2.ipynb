{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8487b4db-5dd3-4aa4-ad1b-ba5d5317ef58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf0e72b7-a80a-4da4-8cf5-4eb3950fd956",
   "metadata": {},
   "source": [
    "# 1. [20 pts] At a high level (i.e., without entering into mathematical details), please describe, compare, and contrast the following classifiers:\n",
    " - Perceptron (textbook's version)\n",
    " - SVM\n",
    " - Decision Tree\n",
    " - Random Forest (you have to research a bit about this classifier)\n",
    "\n",
    "\n",
    "Some comparison criterion can be:\n",
    " - Speed?\n",
    " - Strength?\n",
    " - Robustness?\n",
    " - The feature type that the classifier naturally uses (e.g. relying on distance means that\n",
    "numerical features are naturally used)\n",
    " - Is it statistical?\n",
    " - Does the method solve an optimization problem? If yes, what is the cost function?\n",
    "\n",
    "\n",
    "Which one will be the first that you would try on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e634e-774b-4315-b112-3c98e5fcd5f9",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "With an unknown dataset, I would likely try the SVM first.  If I knew that the dataset had relatively few features (less than say 20), then I would use a decision tree for its speed and simplicity.  I've seen decision trees perform very well on tasks that took deep neural networks incredible amounts of data and time. The following paper is just such an example [6].\n",
    "\n",
    "- Perceptron: Minimizes the error in predictions. It is not statistical, but does solve an optimization (minimization) problem. It's a relatively weak/unrobust algorithm, because it will never converge if data isn't linearly separable.  It doesn't have kernel methods available and thus can't add dimensions to make hyperplanes evident.  The perceptron relies on numeric data.\n",
    "\n",
    "- SVM: algorithm for classification and regression.  Data must be linearly separable unless the \"kernel trick\" is utilized. Maximizes the distance between classes of data. SVMs perform better with high dimension data than decision trees [2]. Can use a variety of methods for optimization-- radial basis function, sigmoid, polynomial, linear. It is not statistical, but does solve optimization (maximization) problem.\n",
    "\n",
    "- Decision tree: an algorithm for classification.  Create a rule for a subset, or all of, the features in the dataset. Great for nominal data and numeric data. The logic of a decision tree is easy to interpret. Doesn't require feature scaling.  Not as robust as Random Forest-- more susceptible to overfitting and doesn't generalize as well [1]. It is stastical in that it's looking for things like averages/means, medians, and modes. Quick and easy to train if the dataset has few features or is small.\n",
    "\n",
    "- Random forest: ensemble learner for classification and regression. Generalizes well due to how trees are generated [1]. It is stastical because it is derived from decision trees. More time consuming to train than a decision tree, since you are generating multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b41d64-e8c9-4d79-aaa9-6c81c1e52a50",
   "metadata": {},
   "source": [
    "# 2. [20 pts] Define the following feature types and give example values from a dataset. You can pull examples from an existing dataset (like the Iris dataset) or you could write out a dataset yourself. (Hint: In order to give examples for each feature type, you will probably have to use more than one dataset.)\n",
    " - Numerical\n",
    " - Nominal\n",
    " - Date\n",
    " - Text\n",
    " - Image\n",
    " - Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85166b7b-5d1b-4339-8a7e-e5c4139624e4",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    " - Numerical: characteristics of features represented by numerical values.  In the Iris dataset `petal length`, `petal width`, `sepal length`, `sepal width` are all numeric features.\n",
    " - Nominal: are descriptive features, without numerica characteristics.  On Kaggle there is a dataset named [Vehicle Dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho).  The `Name` feature contains information on the make, model and trim level of the car, which are each features in themselves.\n",
    " - Date: These features indicate a time related to the sample.  They can be values such as `year`, `month`, `day`, `hour`, `minute`, `second`, `millisecond`, or a combination of all of those.  The [Vehicle Dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho) contains `year`. Another dataset with more date features is [Room Occupancy Estimation](https://archive.ics.uci.edu/dataset/864/room+occupancy+estimation) which contains `date` and `time` features.  `date` is formatted as YYYY/MM/DD, `time` is formatted as HH:MM:SS.\n",
    " - Text: these are sentences, or words.  They are used in tasks like sentiment analysis, or perhaps something like classifying a support ticket as \"hardware issue\" or \"finance issue.\"  An example of a sentiment analysis dataset is the [Customer Feedback Dataset](https://www.kaggle.com/datasets/vishweshsalodkar/customer-feedback-dataset) on Kaggle.  The dataset shows only one feature, `Text, Sentiment, Source, Date/Time, User ID, Location, Confidence Score`, but on inspection we can see that the first field in this single feature is itself a `text` feature.  Another dataset rich with text is the [Support-tickets-classification](https://www.kaggle.com/datasets/aniketg11/supportticketsclassification) dataset on Kaggle.  It contains text features `title` and `body` which can be used to determine what team is responsible for the ticket.\n",
    " - Image: image features are just that--images.  From what I've read it appears these are always transformed into RGB with values (0,255).  An example dataset is the [AI vs. Human-Generated Images](https://www.kaggle.com/datasets/alessandrasala79/ai-vs-human-generated-dataset).  This dataset contains a CSV with relative paths to image files, the feature is called `file_name`.  Another example of images in a dataset is [Olivetti Faces](https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset) which I reviewed on Scikit-Learn's website.  The dataset has an `images` feature which has 400 samples, each of which is 64x64 matrix of grayscale image values.\n",
    " - Dependent variable: this is a variable which is determined by features (or independent variables).  In the Iris dataset these are the species Iris-Setosa, Iris-Versicolour, Iris-Virginica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b756049c-0143-475d-9fb3-3358343e6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Olivetti faces dataset: (400, 4096)\n"
     ]
    }
   ],
   "source": [
    "### Code 2\n",
    "\n",
    "# Directly from https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "olivetti_faces = fetch_olivetti_faces()\n",
    "print(f'Shape of Olivetti faces dataset: {olivetti_faces.data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b4209-5f9b-416d-97e2-7ba92fda9042",
   "metadata": {},
   "source": [
    "# 3. [20 pts] Using online resources, research and find other classifier performance metrics which are also as common as the accuracy metric. Provide the mathematical equations for them and explain in your own words the meaning of the different metrics you found. Note that providing mathematical equations might involve defining some more fundamental terms, e.g. you should define “False Positive,” if you answer with a metric that builds on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ca8fa-d75f-4410-a644-3bfa618a1aff",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "#### Definitions\n",
    "`TP`: True positive, correctly predicted positive samples\n",
    "\n",
    "`FP`: False positive, incorrectly predicted positive samples\n",
    "\n",
    "`TN`: True negative, correctly predicted negative samples\n",
    "\n",
    "`FN`: False negative, incorrectly predicted negative samples\n",
    "\n",
    "`TPR`: True positive rate, the number of true positives divided by the total number of guesses\n",
    "\n",
    "`FPR`: False positive rate, the number of false positives divided by the total number of guesses\n",
    "\n",
    "\n",
    "\n",
    "#### F1 Score\n",
    "Or F-Score, weighs a models ability to pick TP out of everything it picked (precision), and ability to pick TP out of all possible correct answers (recall).  \n",
    "\n",
    "\\begin{equation}\n",
    "F-Score = \\frac{2 \\times TP}{2 \\times TP + FP + FN}\n",
    "\\end{equation}\n",
    "[1], [5], [11]\n",
    "\n",
    "\n",
    "#### Matthews Correlation Coefficient (MCC)\n",
    "This is the Pearson coefficient provided in Module 2's Jupyter notebook, but it is only applicable to binary classifications.\n",
    "\\begin{equation}\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{equation}\n",
    "[1], [12]\n",
    "\n",
    "\n",
    "#### Receiver Operating Characteristic\n",
    "This is a _classifier's_ ability to distinguish between positive and negative cases given a varying decision threshold.  That is to say, if the threshold for making a decision changes, how well does the model perform?  This is measured with `AUC`: area under the curve.  Thinking back to calculus we measure this with an integral.  1 is perfect score, .5 is random guessing, 0 is totally wrong.\n",
    "\n",
    "\\begin{equation}\n",
    "AUC = \\int_0^1TPR d(FPR) \n",
    "\\end{equation}\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80605753-102e-4b9f-b187-cbaea653e749",
   "metadata": {},
   "source": [
    "# 4. [40 pts] Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file. (This Graduate Admission dataset, with 9 features and 500 data points, is not provided on Canvas; you have to download it from Kaggle by following the instructions in the module Jupyter notebook.) Remember, you are not allowed to used numpy functions such as mean(), stdev(), cov(), etc. You may use DataFrame.corr() only to verify the correctness of your from-scratch matrix.\n",
    "## Display the correlation matrix where each row and column are the features. (Hint: this\n",
    "should be an 8 by 8 matrix.)\n",
    " - Should we use 'Serial no'? Why or why not?\n",
    " - Observe that the diagonal of this matrix should have all 1's; why is this?\n",
    " - Since the last column can be used as the target (dependent) variable, what do you\n",
    "think about the correlations between all the variables?\n",
    " - Which variable should be the most important to try to predict 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ed65b-98a8-4ec0-9511-87ac652349ed",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "- We should not use serial number as it has no bearing on the applicant, it's an arbitrarily assigned number.\n",
    "- Because the diagonal is represents each vector, Xi, correlated with itself.  Since these are the same they are perfectly related.\n",
    "- Correlating between all the variables isn't useful. We only need the correlation of each variable with the target column.\n",
    "- The variable most important in trying to predict 'Chance of Admit' is `CGPA` because it has the highest correlation with the target samples.\n",
    "\n",
    "\n",
    "\n",
    "#### Note on my code:\n",
    "I did the correlation coefficient first, and my `correlate_with_target()`. Then realized the question was probably asking for a matrix because one is shown in the book.  In any case, both correlation matrix and correlation coefficients of feature vs target are shown.\n",
    "- functions are defined first\n",
    "- data is read in\n",
    "- my functions are applied, and followed by the analog in Pandas\n",
    "\n",
    "\n",
    "[7], [8], [9], [10], [11], [12] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8159026d-34c4-40c8-9357-148d8fc63171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature                       |\tChance of Admit               \n",
      "-------------------------------------------------------\n",
      "Serial No.                    |\t0.042336                      \n",
      "GRE Score                     |\t0.802610                      \n",
      "TOEFL Score                   |\t0.791594                      \n",
      "University Rating             |\t0.711250                      \n",
      "SOP                           |\t0.675732                      \n",
      "LOR                           |\t0.669889                      \n",
      "CGPA                          |\t0.873289                      \n",
      "Research                      |\t0.553202                      \n",
      "Chance of Admit               |\t1.000000                      \n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Pandas DataFrame.corrwith():\n",
      "--------------------------------------------------\n",
      "Serial No.           0.042336\n",
      "GRE Score            0.802610\n",
      "TOEFL Score          0.791594\n",
      "University Rating    0.711250\n",
      "SOP                  0.675732\n",
      "LOR                  0.669889\n",
      "CGPA                 0.873289\n",
      "Research             0.553202\n",
      "Chance of Admit      1.000000\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Serial No.          \tGRE Score           \tTOEFL Score         \tUniversity Rating   \tSOP                 \tLOR                 \tCGPA                \tResearch            \tChance of Admit     \t\n",
      "Serial No.          \t1.000000            \t-0.097526           \t-0.147932           \t-0.169948           \t-0.166932           \t-0.088221           \t-0.045608           \t-0.063138           \t0.042336            \t\n",
      "GRE Score           \t-0.097526           \t1.000000            \t0.835977            \t0.668976            \t0.612831            \t0.557555            \t0.833060            \t0.580391            \t0.802610            \t\n",
      "TOEFL Score         \t-0.147932           \t0.835977            \t1.000000            \t0.695590            \t0.657981            \t0.567721            \t0.828417            \t0.489858            \t0.791594            \t\n",
      "University Rating   \t-0.169948           \t0.668976            \t0.695590            \t1.000000            \t0.734523            \t0.660123            \t0.746479            \t0.447783            \t0.711250            \t\n",
      "SOP                 \t-0.166932           \t0.612831            \t0.657981            \t0.734523            \t1.000000            \t0.729593            \t0.718144            \t0.444029            \t0.675732            \t\n",
      "LOR                 \t-0.088221           \t0.557555            \t0.567721            \t0.660123            \t0.729593            \t1.000000            \t0.670211            \t0.396859            \t0.669889            \t\n",
      "CGPA                \t-0.045608           \t0.833060            \t0.828417            \t0.746479            \t0.718144            \t0.670211            \t1.000000            \t0.521654            \t0.873289            \t\n",
      "Research            \t-0.063138           \t0.580391            \t0.489858            \t0.447783            \t0.444029            \t0.396859            \t0.521654            \t1.000000            \t0.553202            \t\n",
      "Chance of Admit     \t0.042336            \t0.802610            \t0.791594            \t0.711250            \t0.675732            \t0.669889            \t0.873289            \t0.553202            \t1.000000            \t\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Pandas DataFrame.corr():\n",
      "--------------------------------------------------\n",
      "                   Serial No.  GRE Score  TOEFL Score  University Rating  \\\n",
      "Serial No.           1.000000  -0.097526    -0.147932          -0.169948   \n",
      "GRE Score           -0.097526   1.000000     0.835977           0.668976   \n",
      "TOEFL Score         -0.147932   0.835977     1.000000           0.695590   \n",
      "University Rating   -0.169948   0.668976     0.695590           1.000000   \n",
      "SOP                 -0.166932   0.612831     0.657981           0.734523   \n",
      "LOR                 -0.088221   0.557555     0.567721           0.660123   \n",
      "CGPA                -0.045608   0.833060     0.828417           0.746479   \n",
      "Research            -0.063138   0.580391     0.489858           0.447783   \n",
      "Chance of Admit      0.042336   0.802610     0.791594           0.711250   \n",
      "\n",
      "                        SOP      LOR       CGPA  Research  Chance of Admit   \n",
      "Serial No.        -0.166932 -0.088221 -0.045608 -0.063138          0.042336  \n",
      "GRE Score          0.612831  0.557555  0.833060  0.580391          0.802610  \n",
      "TOEFL Score        0.657981  0.567721  0.828417  0.489858          0.791594  \n",
      "University Rating  0.734523  0.660123  0.746479  0.447783          0.711250  \n",
      "SOP                1.000000  0.729593  0.718144  0.444029          0.675732  \n",
      "LOR                0.729593  1.000000  0.670211  0.396859          0.669889  \n",
      "CGPA               0.718144  0.670211  1.000000  0.521654          0.873289  \n",
      "Research           0.444029  0.396859  0.521654  1.000000          0.553202  \n",
      "Chance of Admit    0.675732  0.669889  0.873289  0.553202          1.000000  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Note to the grader: \n",
    "    - functions are defined first\n",
    "    - data is read in\n",
    "    - my functions are applied, and followed by the analog in Pandas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vector_checker(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns TRUE if the vectors are of the same size, FALSE otherwise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    vec2 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    if len(vec1) != len(vec2):\n",
    "        print(f\"Input vectors must be of same length. Recieved vec1 of length {len(vec1)}; vec2 of length {len(vec2)}.\")\n",
    "        return False\n",
    "    if len(vec1) == 0:\n",
    "        print(f\"Vectors must have non-zero length\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean(vec):\n",
    "    \"\"\"\n",
    "    Returns the mean, or expected value I guess, of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    return np.sum(vec)/len(vec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def std_dev(vec):\n",
    "    \"\"\"\n",
    "    Returns the standard deviation of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    mu = mean(vec)\n",
    "    ret = (vec - mu)**2\n",
    "    ret = ret.sum()\n",
    "    ret = np.sqrt(ret / len(vec))\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def variance(vec):\n",
    "    \"\"\"\n",
    "    Returns the variance of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    mu = mean(vec)\n",
    "    n = len(vec)\n",
    "    \n",
    "    ret = 0\n",
    "    for i in range(len(vec)):\n",
    "        ret += (vec[i] - mu)**2 / n\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def covariance(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns the covariance of vectors vec1 and vec2 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    vec2 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    if not vector_checker(vec1, vec2):\n",
    "        return \"N/A\"\n",
    "\n",
    "    count = len(vec1)\n",
    "    mu_vec1 = mean(vec1)\n",
    "    mu_vec2 = mean(vec2)\n",
    "    ret = 0\n",
    "    for i in range(count):\n",
    "        ret += (vec1[i] - mu_vec1)*(vec2[i] - mu_vec2)\n",
    "    return ret / (count-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns the dot product of vectors vec1 and vec2 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    vec2 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    if vector_checker(vec1, vec2) == False:\n",
    "        return \"N/A\"\n",
    "    product = 0\n",
    "    for i in range(len(vec1)):\n",
    "        product += vec1[i] *  vec2[i]\n",
    "    return product\n",
    "\n",
    "\n",
    "def correlation_coefficient(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns the Pearson correlation coefficient for vec1 and vec2.  Uses the formula at\n",
    "    https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    vec2 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pearson's correlation coefficient : float\n",
    "    \"\"\"\n",
    "    mu_vec1 = mean(vec1)\n",
    "    mu_vec2 = mean(vec2)\n",
    "    numerator = dot_product(vec1 - mu_vec1, vec2 - mu_vec2)\n",
    "    denominator = ( np.sum( (vec1 - mu_vec1)**2 ) )**.5 * ( np.sum( (vec2 - mu_vec2)**2 ) )**.5\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def correlate_with_target(feature_data:np.ndarray=None, feature_labels:list=None, target:np.ndarray=None, target_label:list=None):\n",
    "    \"\"\"\n",
    "    Prints each feature name and its correlation with the target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_data : {numpy.ndarray}, shape = [n, m]\n",
    "        An array of feature data, where each feature is in a column, and each row is a sample\n",
    "\n",
    "    feature_labels: list {str}, shape = [1,m]\n",
    "        A list of feature names, as strings\n",
    "\n",
    "    target : {numpy.ndarray}, shape = [n, 1]\n",
    "        A vector of the target data\n",
    "\n",
    "    target_label: list {str}, shape = [1,1]\n",
    "        A list of a single string, the target column name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print('{:<30}'.format('Feature'), end='|\\t')\n",
    "    print('{:<30}'.format(target_label))\n",
    "    print('-' * 55)\n",
    "    for i in range(len(feature_labels)):\n",
    "        print(f'{feature_labels[i]:<30}', end='|\\t')\n",
    "        col = feature_data[:,i]\n",
    "        print(f'{correlation_coefficient(col,target):<30.6f}')\n",
    "    print(f'{target_label:<30}', end='|\\t')\n",
    "    print(f'{correlation_coefficient(target,target):<30.6f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def correlation_matrix(data:np.ndarray=None, labels:list=None):\n",
    "    \"\"\"\n",
    "    Prints the correlation matrix for every column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : {numpy.ndarray}, shape = [n, m]\n",
    "        An array of feature data, where each feature is in a column, and each row is a sample\n",
    "\n",
    "    labels: list {str}, shape = [1,m]\n",
    "        A list of column names, as strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f'{\"\":<20}', end='')\n",
    "    for i in labels:\n",
    "        print(f'{i:<20}', end='\\t')\n",
    "    print()\n",
    "    # rows\n",
    "    for i in range(len(labels)):\n",
    "        # columns\n",
    "        for j in range(len(labels)):\n",
    "            if j == 0:\n",
    "                print(f'{labels[i]:<20}', end='\\t')\n",
    "            s = 'm'\n",
    "            v1 = data[:,i]\n",
    "            v2 = data[:,j]\n",
    "            print(f'{correlation_coefficient(v1,v2):<20.6f}', end='\\t')\n",
    "        print()\n",
    "            \n",
    "    return None\n",
    "\n",
    "## Read the data\n",
    "valid = pd.read_csv('datasets/Admission_Predict.csv')\n",
    "labels = valid.columns.tolist()\n",
    "data = np.loadtxt('datasets/Admission_Predict.csv',delimiter=',', skiprows=1)\n",
    "feature_data = data[:,:8]\n",
    "feature_labels = labels[:8]\n",
    "target = data[:,data.shape[1]-1]\n",
    "target_label = labels[len(labels)-1]\n",
    "\n",
    "\n",
    "## Correlate each feature with the target\n",
    "correlate_with_target(feature_data=feature_data, feature_labels=feature_labels, target=target, target_label=target_label)\n",
    "df = pd.DataFrame(data, columns=labels)\n",
    "print(f'\\n\\n{\"-\"*50}\\nPandas DataFrame.corrwith():\\n{\"-\"*50}\\n{df.corrwith(df[\"Chance of Admit \"])}')\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "## Print the correlation matrix\n",
    "correlation_matrix(data,labels)\n",
    "print(f'\\n\\n{\"-\"*50}\\nPandas DataFrame.corr():\\n{\"-\"*50}\\n{df.corr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28c2c7-a6a1-41b6-bf27-1f0a6c18482a",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Raschka, Sebastian, et al. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022.\n",
    "2. IBM. What are support vector machines (SVMs)? https://www.ibm.com/think/topics/support-vector-machine. Last accessed 1 February, 2025.\n",
    "3. IBM. What is random forest? https://www.ibm.com/think/topics/random-forest.  Last accessed 1 February, 2025.\n",
    "4. IBM. What is a decision tree? https://www.ibm.com/think/topics/decision-trees. Last accessed 1 February, 2025.\n",
    "5. Scikit-Learn. https://scikit-learn.org/stable/.\n",
    "6. Azzouz, Elsayed Elsayed, and Asoke K. Nandi. \"Automatic identification of digital modulation types.\" Signal processing 47.1 (1995): 55-69.\n",
    "7. Wikipedia. Covariance. https://en.wikipedia.org. Last accessed 3 February 2025.\n",
    "8. Wikipedia. Pearson Correlation Coefficient. https://en.wikipedia.org. Last accessed 3 February 2025.\n",
    "9. Wikipedia. Cross-Correlation. https://en.wikipedia.org. Last accessed 3 February 2025.\n",
    "10. Wikipedia. Correlation. https://en.wikipedia.org. Last accessed 3 February 2025.\n",
    "11. Wikipedia. F-Score. https://en.wikipedia.org. Last accessed 3 February 2025.\n",
    "12. Wikipedia. Matthews Correlation Coefficient. https://en.wikipedia.org. Last accessed 3 February 2025.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
