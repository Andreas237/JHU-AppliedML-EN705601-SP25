{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8487b4db-5dd3-4aa4-ad1b-ba5d5317ef58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf0e72b7-a80a-4da4-8cf5-4eb3950fd956",
   "metadata": {},
   "source": [
    "# 1. [20 pts] At a high level (i.e., without entering into mathematical details), please describe, compare, and contrast the following classifiers:\n",
    " - Perceptron (textbook's version)\n",
    " - SVM\n",
    " - Decision Tree\n",
    " - Random Forest (you have to research a bit about this classifier)\n",
    "\n",
    "\n",
    "Some comparison criterion can be:\n",
    " - Speed?\n",
    " - Strength?\n",
    " - Robustness?\n",
    " - The feature type that the classifier naturally uses (e.g. relying on distance means that\n",
    "numerical features are naturally used)\n",
    " - Is it statistical?\n",
    " - Does the method solve an optimization problem? If yes, what is the cost function?\n",
    "\n",
    "\n",
    "Which one will be the first that you would try on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e634e-774b-4315-b112-3c98e5fcd5f9",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "With an unknown dataset, I would likely try the SVM first.  If I knew that the dataset had relatively few features (less than say 20), then I would use a decision tree for its speed and simplicity.  I've seen decision trees perform very well on tasks that took deep neural networks incredible amounts of data and time. The following paper is just such an example [6].\n",
    "\n",
    "- Perceptron: Minimizes the error in predictions. It is not statistical, but does solve an optimization (minimization) problem. It's a relatively weak/unrobust algorithm, because it will never converge if data isn't linearly separable.  It doesn't have kernel methods available and thus can't add dimensions to make hyperplanes evident.  The perceptron relies on numeric data.\n",
    "\n",
    "- SVM: algorithm for classification and regression.  Data must be linearly separable unless the \"kernel trick\" is utilized. Maximizes the distance between classes of data. SVMs perform better with high dimension data than decision trees [2]. Can use a variety of methods for optimization-- radial basis function, sigmoid, polynomial, linear. It is not statistical, but does solve optimization (maximization) problem.\n",
    "\n",
    "- Decision tree: an algorithm for classification.  Create a rule for a subset, or all of, the features in the dataset. Great for nominal data and numeric data. The logic of a decision tree is easy to interpret. Doesn't require feature scaling.  Not as robust as Random Forest-- more susceptible to overfitting and doesn't generalize as well [1]. It is stastical in that it's looking for things like averages/means, medians, and modes. Quick and easy to train if the dataset has few features or is small.\n",
    "\n",
    "- Random forest: ensemble learner for classification and regression. Generalizes well due to how trees are generated [1]. It is stastical because it is derived from decision trees. More time consuming to train than a decision tree, since you are generating multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b41d64-e8c9-4d79-aaa9-6c81c1e52a50",
   "metadata": {},
   "source": [
    "# 2. [20 pts] Define the following feature types and give example values from a dataset. You can pull examples from an existing dataset (like the Iris dataset) or you could write out a dataset yourself. (Hint: In order to give examples for each feature type, you will probably have to use more than one dataset.)\n",
    " - Numerical\n",
    " - Nominal\n",
    " - Date\n",
    " - Text\n",
    " - Image\n",
    " - Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85166b7b-5d1b-4339-8a7e-e5c4139624e4",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    " - Numerical: characteristics of features represented by numerical values.  In the Iris dataset `petal length`, `petal width`, `sepal length`, `sepal width` are all numeric features.\n",
    " - Nominal: are descriptive features, without numerica characteristics.  On Kaggle there is a dataset named [Vehicle Dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho).  The `Name` feature contains information on the make, model and trim level of the car, which are each features in themselves.\n",
    " - Date: These features indicate a time related to the sample.  They can be values such as `year`, `month`, `day`, `hour`, `minute`, `second`, `millisecond`, or a combination of all of those.  The [Vehicle Dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho) contains `year`. Another dataset with more date features is [Room Occupancy Estimation](https://archive.ics.uci.edu/dataset/864/room+occupancy+estimation) which contains `date` and `time` features.  `date` is formatted as YYYY/MM/DD, `time` is formatted as HH:MM:SS.\n",
    " - Text: these are sentences, or words.  They are used in tasks like sentiment analysis, or perhaps something like classifying a support ticket as \"hardware issue\" or \"finance issue.\"  An example of a sentiment analysis dataset is the [Customer Feedback Dataset](https://www.kaggle.com/datasets/vishweshsalodkar/customer-feedback-dataset) on Kaggle.  The dataset shows only one feature, `Text, Sentiment, Source, Date/Time, User ID, Location, Confidence Score`, but on inspection we can see that the first field in this single feature is itself a `text` feature.  Another dataset rich with text is the [Support-tickets-classification](https://www.kaggle.com/datasets/aniketg11/supportticketsclassification) dataset on Kaggle.  It contains text features `title` and `body` which can be used to determine what team is responsible for the ticket.\n",
    " - Image: image features are just that--images.  From what I've read it appears these are always transformed into RGB with values (0,255).  An example dataset is the [AI vs. Human-Generated Images](https://www.kaggle.com/datasets/alessandrasala79/ai-vs-human-generated-dataset).  This dataset contains a CSV with relative paths to image files, the feature is called `file_name`.  Another example of images in a dataset is [Olivetti Faces](https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset) which I reviewed on Scikit-Learn's website.  The dataset has an `images` feature which has 400 samples, each of which is 64x64 matrix of grayscale image values.\n",
    " - Dependent variable: this is a variable which is determined by features (or independent variables).  In the Iris dataset these are the species Iris-Setosa, Iris-Versicolour, Iris-Virginica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b756049c-0143-475d-9fb3-3358343e6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Olivetti faces dataset: (400, 4096)\n"
     ]
    }
   ],
   "source": [
    "### Code 2\n",
    "\n",
    "# Directly from https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "olivetti_faces = fetch_olivetti_faces()\n",
    "print(f'Shape of Olivetti faces dataset: {olivetti_faces.data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b4209-5f9b-416d-97e2-7ba92fda9042",
   "metadata": {},
   "source": [
    "# 3. [20 pts] Using online resources, research and find other classifier performance metrics which are also as common as the accuracy metric. Provide the mathematical equations for them and explain in your own words the meaning of the different metrics you found. Note that providing mathematical equations might involve defining some more fundamental terms, e.g. you should define “False Positive,” if you answer with a metric that builds on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ca8fa-d75f-4410-a644-3bfa618a1aff",
   "metadata": {},
   "source": [
    "### Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80605753-102e-4b9f-b187-cbaea653e749",
   "metadata": {},
   "source": [
    "# 4. [40 pts] Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file. (This Graduate Admission dataset, with 9 features and 500 data points, is not provided on Canvas; you have to download it from Kaggle by following the instructions in the module Jupyter notebook.) Remember, you are not allowed to used numpy functions such as mean(), stdev(), cov(), etc. You may use DataFrame.corr() only to verify the correctness of your from-scratch matrix.\n",
    "## Display the correlation matrix where each row and column are the features. (Hint: this\n",
    "should be an 8 by 8 matrix.)\n",
    " - Should we use 'Serial no'? Why or why not?\n",
    " - Observe that the diagonal of this matrix should have all 1's; why is this?\n",
    " - Since the last column can be used as the target (dependent) variable, what do you\n",
    "think about the correlations between all the variables?\n",
    " - Which variable should be the most important to try to predict 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8159026d-34c4-40c8-9357-148d8fc63171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Research', 'Chance of Admit ']\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "True False True\n",
      "422.2222222222222 422.22222222222223\n",
      "228.66666666666666\n",
      "          X         Y         Z\n",
      "X  1.000000  0.733273  0.188982\n",
      "Y  0.733273  1.000000  0.806258\n",
      "Z  0.188982  0.806258  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "valid = pd.read_csv('datasets/Admission_Predict.csv')\n",
    "# print(valid.head())\n",
    "# print();print()\n",
    "# print(valid.cov())\n",
    "\n",
    "headers = valid.columns.tolist()\n",
    "print(headers)\n",
    "data = np.loadtxt('datasets/Admission_Predict.csv',delimiter=',', skiprows=1)\n",
    "\n",
    "def mean(vec: np.ndarray):\n",
    "    vec = np.ravel(vec)\n",
    "    return vec.sum() / len(vec)\n",
    "\n",
    "def cov(v1, v2):\n",
    "    \"\"\"\n",
    "    Return the covariance of the two vectors.\n",
    "\n",
    "    Covariance as defined in Wikipedia, https://en.wikipedia.org/wiki/Covariance_matrix\n",
    "    mean[v1 - mean(v1)] * mean[v2 - mean(v2)]\n",
    "    \"\"\"\n",
    "    def get_mean_mean(v):\n",
    "        v = np.ravel(v)\n",
    "        v_count = len(v)\n",
    "        # get inner_mean\n",
    "        sumsofar = 0\n",
    "        for i in v:\n",
    "            sumsofar += i\n",
    "        mu_inner = sumsofar / v_count\n",
    "        \n",
    "\n",
    "        \n",
    "    v1 = np.ravel(v1)\n",
    "    v1_len = len(v1)\n",
    "    v2 = np.ravel(v2)\n",
    "    v2_len = len(v2)\n",
    "    \n",
    "    mu_v1 = v1.sum() / v1_len\n",
    "    mu_v2 = v2.sum() / v2_len\n",
    "\n",
    "    return mean(v1 - mean(v1)) * mean(v2 - mean(v2))\n",
    "\n",
    "c1= data[:,1]\n",
    "c2 = data[:,2]\n",
    "\n",
    "def mean(vec):\n",
    "    \"\"\"\n",
    "    Returns the mean of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stddev : float\n",
    "    \"\"\"\n",
    "    return np.sum(vec)/len(vec)\n",
    "\n",
    "def summer(vec):\n",
    "    \"\"\"\n",
    "    Returns the sum of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stddev : float\n",
    "    \"\"\"\n",
    "    ret = 0\n",
    "    for v in vec:\n",
    "        ret += v\n",
    "    return ret\n",
    "    \n",
    "def std_dev(vec):\n",
    "    \"\"\"\n",
    "    Returns the standard deviation of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    mu = mean(vec)\n",
    "    ret = (vec - mu)**2\n",
    "    ret = summer(ret)\n",
    "    ret = np.sqrt(ret / len(vec))\n",
    "    return ret\n",
    "\n",
    "def variance(vec):\n",
    "    \"\"\"\n",
    "    Returns the variance of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    mu = mean(vec)\n",
    "    n = len(vec)\n",
    "    \n",
    "    ret = 0\n",
    "    for v in vec:\n",
    "        ret += (v - mu)**2 / n\n",
    "    return ret\n",
    "\n",
    "def covariance(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns the covariance of vectors vec1 and vec2 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    vec2 : {numpy.ndarray}, shape = [n, 1]\n",
    "      A vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "    \"\"\"\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(f\"Input vectors must be of same length. Recieved vec1 of length {len(vec1)}; vec2 of length {len(vec2)}.\")\n",
    "        return -1\n",
    "    if len(vec1) == 0:\n",
    "        raise ValueError(f\"Vectors must have non-zero length\")\n",
    "        return -1\n",
    "\n",
    "    count = len(vec1)\n",
    "    mu_vec1 = mean(vec1)\n",
    "    mu_vec2 = mean(vec2)\n",
    "    ret = 0\n",
    "    for i in range(count):\n",
    "        ret += (vec1[i] - mu_vec1)*(vec2[i] - mu_vec2)\n",
    "    return ret / count\n",
    "\n",
    "\n",
    "def covvar_mat(mat):\n",
    "    \"\"\"\n",
    "    Returns the covariance matrix of vectors vec1 and vec2 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : {numpy.ndarray}, shape = [n, m]\n",
    "      A matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : {numpy.ndarray}, shape = [n, m]\n",
    "    \"\"\"\n",
    "    ret = np.array(mat.shape)\n",
    "    return None\n",
    "\n",
    "\n",
    "test0 = np.array([80,63,100])\n",
    "test1 = np.array([70,20,50])\n",
    "test2 = np.array([60,30,90])\n",
    "print(mean(test0) == np.mean(test0), mean(test1) == np.mean(test1)) \n",
    "print(summer(test0) == np.sum(test0), summer(test1) == np.sum(test1))\n",
    "print(std_dev(test0) == np.std(test0), std_dev(test1) == np.std(test1))\n",
    "print(variance(test0) == np.var(test0), variance(test1) == np.var(test1), variance(test2) == np.var(test2))\n",
    "print(variance(test1), np.var(test1))\n",
    "print(covvar(test, test))\n",
    "# print(np.cov(test))\n",
    "df = pd.DataFrame(data=[test0,test1,test2], columns=['X','Y','Z'])\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28c2c7-a6a1-41b6-bf27-1f0a6c18482a",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Raschka, Sebastian, et al. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022.\n",
    "2. IBM. What are support vector machines (SVMs)? https://www.ibm.com/think/topics/support-vector-machine. Last accessed 1 February, 2025.\n",
    "3. IBM. What is random forest? https://www.ibm.com/think/topics/random-forest.  Last accessed 1 February, 2025.\n",
    "4. IBM. What is a decision tree? https://www.ibm.com/think/topics/decision-trees. Last accessed 1 February, 2025.\n",
    "5. Scikit-Learn. https://scikit-learn.org/stable/.\n",
    "6. Azzouz, Elsayed Elsayed, and Asoke K. Nandi. \"Automatic identification of digital modulation types.\" Signal processing 47.1 (1995): 55-69.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
