{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75892bad-b92b-4b52-84f6-5fa76cb410ed",
   "metadata": {},
   "source": [
    "## 1. [20 pts] Define each of the following machine learning terms in your own words:\n",
    "  1. the training dataset, testing dataset, and validation dataset\n",
    "\n",
    "    a. Training dataset: the dataset used to \"teach\" the model how to respond to a certain format of data.  More specifically, the dataset used to generate weights for the model.\n",
    "    b. Testing dataset\n",
    "    c. Validation dataset: \n",
    "\n",
    "    \n",
    "     \n",
    "  3. ground truth, label\n",
    "  4. pre-processing, feature, numerical, nominal\n",
    "  5. decision surface\n",
    "  6. model validation, accuracy, cross-validation\n",
    "  7. parameters, hyperparameters, overfit\n",
    "  8. reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a778be8-c0d2-411d-8266-f28a692c8946",
   "metadata": {},
   "source": [
    "## 2. [12 pts] Pick the Iris dataset of the Scikit-learn datasets for classification which is included in the library (i.e. the dataset can be loaded with datasets.load_) and find out the following:\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79f6cc-6f1e-4e39-9c69-de935c0df9f9",
   "metadata": {},
   "source": [
    "## 3. [8 pts] Next, locate the Wine dataset, load it, and explore it to find out the same characteristics (as in #2.i-iv) once more.\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373b322-4cff-4db3-afec-c3efaf53bdb1",
   "metadata": {},
   "source": [
    "## 4. [20 pts] Use the following lines of code to display feature pairs from the Iris dataset:\n",
    "```\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "iris = sklearn.datasets.load_iris()\n",
    "iris_df = pd.DataFrame(\n",
    "data= np.c_[iris.data, [iris.target_names[v] for v in iris.target]],\n",
    "columns= iris.feature_names + ['species'])\n",
    "cols = iris_df.columns.drop('species')\n",
    "iris_df[cols] = iris_df[cols].apply(pd.to_numeric)\n",
    "g = sns.pairplot(iris_df, hue='species')\n",
    "```\n",
    "From the plots, which feature(s) shows the most promising separation power for machine\n",
    "learning?\n",
    "Now plot the features of the Wine dataset in question 2. When there are too many features,\n",
    "it is possible to switch the dataset or update your code (pandas Dataframe line) to look at\n",
    "only a certain number of features at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f5d97-fad8-433c-b4f8-acb6a29fc9ed",
   "metadata": {},
   "source": [
    "## 5. [20 pts] Consider the Iris dataset, refer to the plots in the previous question, and discuss/think/outline an unsupervised approach to group the dataset into non-overlapping clusters. Answer the following questions:\n",
    "  1. Which features would you use?\n",
    "  2. Are three clusters obvious from the plots?\n",
    "  3. What about four clusters? Either roughly mark them manually (i.e. indicating cutoff thresholds) on a few plots if possible, or specify their ranges (i.e. provide the range of values per cluster per each feature youâ€™ve chosen to include).\n",
    "  4. For this problem, is there any relation between classification and clustering since the labels are already given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899e3d6-f2d2-4bc1-9f26-7cb621e4e9c5",
   "metadata": {},
   "source": [
    "## 6. [20 pts] Using the scikit-learn class descriptions for Naive Bayes and decision trees, classify the Iris dataset from question 4. Your code should be very similar to that in the Module 1 Jupyter notebook. Make sure to divide the dataset into two portions, one for training and the other for testing, as is done in that notebook, using sklearn.model_selection.train_test_split (and make sure that shuffle is on). Then, answer the following questions:\n",
    "  1. Why is using shuffle important for this dataset?\n",
    "  2. Which classifier has the higher performance?\n",
    "  3. Does more training help? Test this by increasing the training dataset size from let's say 1% training (with the remaining 99% testing), and then 5% training, 10% training, etc.\n",
    "  4. Will the performance plateau? Show it on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4eb41-e454-4895-9294-0e5f5e005bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
