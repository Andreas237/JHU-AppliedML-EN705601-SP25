{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75892bad-b92b-4b52-84f6-5fa76cb410ed",
   "metadata": {},
   "source": [
    "## 1. [20 pts] Define each of the following machine learning terms in your own words:\n",
    "  1. the training dataset, testing dataset, and validation dataset \n",
    "  2. ground truth, label\n",
    "  3. pre-processing, feature, numerical, nominal\n",
    "  4. decision surface\n",
    "  5. model validation, accuracy, cross-validation\n",
    "  6. parameters, hyperparameters, overfit\n",
    "  7. reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482ac21-9b41-4450-9778-455a809f8f8b",
   "metadata": {},
   "source": [
    "### Answers to 1.\n",
    "1. `Training dataset`: the dataset used to \"teach\" the model how to respond to a certain format of data.  More specifically, the dataset used to generate weights for the model. `Testing dataset`: a dataset the model has not seen before, to which the model is applied. `Validation dataset`: a dataset that the model has not been trained on, but that is used to measure error at every epoch.\n",
    "\n",
    "2. `Ground truth`: . `Label`:                                                             \n",
    "\n",
    "3. `pre-processing` changing the dataset to suit our training algorithm-- like one-hot encoding categorical features, or \"standardizing\" features so that they are all in a similar numeric range like 0 to 1. `feature` an aspect of the dataset that we want to consider when training a model. `numerical` is a feature represented by a number. `nominal` is a feature that is non-numeric, could be called a categorical feature; like \"vegetable\" or \"fruit.\"\n",
    "\n",
    "4. a\n",
    "\n",
    "5. a\n",
    "\n",
    "6. `Parameters` are the weights and biases internal to the model. `Hyperparameters` are the nerd knobs that can be turned to change how the model is trained; per the textbook-- things like training rate and epochs. `Overfit` is when a model is only well suited to providing insights into the data it was trained on.\n",
    "\n",
    "7. a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a778be8-c0d2-411d-8266-f28a692c8946",
   "metadata": {},
   "source": [
    "## 2. [12 pts] Pick the Iris dataset of the Scikit-learn datasets for classification which is included in the library (i.e. the dataset can be loaded with datasets.load_) and find out the following:\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb564d44-48bc-4a63-848b-a28764dfdb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the answers come first.  The code I used to inspect the data is at the bottom of this cell, commented out to reduce clutter when the cell is run.\n",
      "\n",
      "\n",
      "1)\n",
      "Number of items in the dataset: 150\n",
      "\n",
      "\n",
      "2)\n",
      "Feature names and types:\n",
      "sepal length (cm)    float64\n",
      "sepal width (cm)     float64\n",
      "petal length (cm)    float64\n",
      "petal width (cm)     float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "3)\n",
      "There are 3 categories with names names ['setosa' 'versicolor' 'virginica'], and types int64\n",
      "\n",
      "\n",
      "4)\n",
      "Mean of the first two features (which are not nominal):\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'Note: the answers come first.  The code I used to inspect the data is at the bottom of this cell, commented out to reduce clutter when the cell is run.')\n",
    "\n",
    "from sklearn import datasets\n",
    "ds = datasets.load_iris(as_frame=True)\n",
    "\n",
    "print(f'\\n\\n1)\\nNumber of items in the dataset: {ds.data.shape[0]}')\n",
    "\n",
    "print(f'\\n\\n2)\\nFeature names and types:\\n{ds.data.dtypes}')\n",
    "\n",
    "print(f'\\n\\n3)\\nThere are {len(ds.target.unique())} categories with names names {ds.target_names}, and types {ds.target.dtype}')\n",
    "\n",
    "# Need True for first two columns, otherwise false\n",
    "yes_cols = [True if col_num < 2 else False for col_num in range(len(ds.feature_names))]\n",
    "print(f'\\n\\n4)\\nMean of the first two features (which are not nominal):\\n{ds.data.iloc[:,yes_cols].mean()}')\n",
    "\n",
    "# print('\\n\\n')\n",
    "# print(ds.keys())\n",
    "# print(ds.data.info(verbose=True))\n",
    "# print(ds.data.dtypes[:2])\n",
    "# print(f'\\n\\nFeature names: {ds.feature_names}')\n",
    "# print(f'\\n\\n4)\\t The means of the first two features are \"{first_two_features.mean()}\" mode: {ds.data[first_two_features[0]].mode()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79f6cc-6f1e-4e39-9c69-de935c0df9f9",
   "metadata": {},
   "source": [
    "## 3. [8 pts] Next, locate the Wine dataset, load it, and explore it to find out the same characteristics (as in #2.i-iv) once more.\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17560d27-05d6-4063-a572-7ffcd2a72ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1)\n",
      "Number of items in the dataset: 178\n",
      "\n",
      "\n",
      "2)\n",
      "Feature names and types:\n",
      "alcohol                         float64\n",
      "malic_acid                      float64\n",
      "ash                             float64\n",
      "alcalinity_of_ash               float64\n",
      "magnesium                       float64\n",
      "total_phenols                   float64\n",
      "flavanoids                      float64\n",
      "nonflavanoid_phenols            float64\n",
      "proanthocyanins                 float64\n",
      "color_intensity                 float64\n",
      "hue                             float64\n",
      "od280/od315_of_diluted_wines    float64\n",
      "proline                         float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "3)\n",
      "There are 3 categories with names names ['class_0' 'class_1' 'class_2'], and types int64\n",
      "\n",
      "\n",
      "4)\n",
      "Mean of the first two features (which are not nominal):\n",
      "alcohol       13.000618\n",
      "malic_acid     2.336348\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "ds = datasets.load_wine(as_frame=True)\n",
    "\n",
    "print(f'\\n\\n1)\\nNumber of items in the dataset: {ds.data.shape[0]}')\n",
    "print(f'\\n\\n2)\\nFeature names and types:\\n{ds.data.dtypes}')\n",
    "print(f'\\n\\n3)\\nThere are {len(ds.target.unique())} categories with names names {ds.target_names}, and types {ds.target.dtype}')\n",
    "# Need True for first two columns, otherwise false\n",
    "yes_cols = [True if col_num < 2 else False for col_num in range(len(ds.feature_names))]\n",
    "print(f'\\n\\n4)\\nMean of the first two features (which are not nominal):\\n{ds.data.iloc[:,yes_cols].mean()}')\n",
    "\n",
    "# print('\\n\\n')\n",
    "# print(ds.keys())\n",
    "# print(ds.data.info(verbose=True))\n",
    "# print(ds.data.dtypes[:2])\n",
    "# print(f'\\n\\nFeature names: {ds.feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373b322-4cff-4db3-afec-c3efaf53bdb1",
   "metadata": {},
   "source": [
    "## 4. [20 pts] Use the following lines of code to display feature pairs from the Iris dataset:\n",
    "```\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "iris = sklearn.datasets.load_iris()\n",
    "iris_df = pd.DataFrame(\n",
    "data= np.c_[iris.data, [iris.target_names[v] for v in iris.target]],\n",
    "columns= iris.feature_names + ['species'])\n",
    "cols = iris_df.columns.drop('species')\n",
    "iris_df[cols] = iris_df[cols].apply(pd.to_numeric)\n",
    "g = sns.pairplot(iris_df, hue='species')\n",
    "```\n",
    "From the plots, which feature(s) shows the most promising separation power for machine\n",
    "learning?\n",
    "Now plot the features of the Wine dataset in question 2. When there are too many features,\n",
    "it is possible to switch the dataset or update your code (pandas Dataframe line) to look at\n",
    "only a certain number of features at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645e54b9-6af2-4d73-ae76-c6baecb05caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_version_predates' from 'seaborn.utils' (/Users/ace/miniconda3/lib/python3.12/site-packages/seaborn/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m; sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticks\u001b[39m\u001b[38;5;124m\"\u001b[39m, color_codes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/seaborn/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmiscplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/seaborn/matrix.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grid\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     despine,\n\u001b[0;32m---> 16\u001b[0m     axis_ticklabels_overlap,\n\u001b[1;32m     17\u001b[0m     relative_luminance,\n\u001b[1;32m     18\u001b[0m     to_utf8,\n\u001b[1;32m     19\u001b[0m     _draw_figure,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_positional_args\n\u001b[1;32m     24\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheatmap\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclustermap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/seaborn/cm.py:2\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/seaborn/_compat.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Figure\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _version_predates\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm_from_scale\u001b[39m(scale, norm):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Produce a Normalize object given a Scale and min/max domain limits.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_version_predates' from 'seaborn.utils' (/Users/ace/miniconda3/lib/python3.12/site-packages/seaborn/utils.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "iris = sklearn.datasets.load_iris()\n",
    "iris_df = pd.DataFrame(\n",
    "data= np.c_[iris.data, [iris.target_names[v] for v in iris.target]],\n",
    "columns= iris.feature_names + ['species'])\n",
    "cols = iris_df.columns.drop('species')\n",
    "iris_df[cols] = iris_df[cols].apply(pd.to_numeric)\n",
    "g = sns.pairplot(iris_df, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f5d97-fad8-433c-b4f8-acb6a29fc9ed",
   "metadata": {},
   "source": [
    "## 5. [20 pts] Consider the Iris dataset, refer to the plots in the previous question, and discuss/think/outline an unsupervised approach to group the dataset into non-overlapping clusters. Answer the following questions:\n",
    "  1. Which features would you use?\n",
    "  2. Are three clusters obvious from the plots?\n",
    "  3. What about four clusters? Either roughly mark them manually (i.e. indicating cutoff thresholds) on a few plots if possible, or specify their ranges (i.e. provide the range of values per cluster per each feature youâ€™ve chosen to include).\n",
    "  4. For this problem, is there any relation between classification and clustering since the labels are already given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899e3d6-f2d2-4bc1-9f26-7cb621e4e9c5",
   "metadata": {},
   "source": [
    "## 6. [20 pts] Using the scikit-learn class descriptions for Naive Bayes and decision trees, classify the Iris dataset from question 4. Your code should be very similar to that in the Module 1 Jupyter notebook. Make sure to divide the dataset into two portions, one for training and the other for testing, as is done in that notebook, using sklearn.model_selection.train_test_split (and make sure that shuffle is on). Then, answer the following questions:\n",
    "  1. Why is using shuffle important for this dataset?\n",
    "  2. Which classifier has the higher performance?\n",
    "  3. Does more training help? Test this by increasing the training dataset size from let's say 1% training (with the remaining 99% testing), and then 5% training, 10% training, etc.\n",
    "  4. Will the performance plateau? Show it on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4eb41-e454-4895-9294-0e5f5e005bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
