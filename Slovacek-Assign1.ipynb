{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75892bad-b92b-4b52-84f6-5fa76cb410ed",
   "metadata": {},
   "source": [
    "## 1. [20 pts] Define each of the following machine learning terms in your own words:\n",
    "  1. the training dataset, testing dataset, and validation dataset \n",
    "  2. ground truth, label\n",
    "  3. pre-processing, feature, numerical, nominal\n",
    "  4. decision surface\n",
    "  5. model validation, accuracy, cross-validation\n",
    "  6. parameters, hyperparameters, overfit\n",
    "  7. reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482ac21-9b41-4450-9778-455a809f8f8b",
   "metadata": {},
   "source": [
    "### Answers to 1.\n",
    "1. `Training dataset`: the dataset used to \"teach\" the model how to respond to a certain format of data.  More specifically, the dataset used to generate weights for the model. `Testing dataset`: a dataset the model has not seen before, to which the model is applied. `Validation dataset`: a dataset that the model has not been trained on, but that is used to measure error at every epoch.\n",
    "\n",
    "2. `Ground truth`: . `Label`:                                                             \n",
    "\n",
    "3. a\n",
    "\n",
    "4. a\n",
    "\n",
    "5. a\n",
    "\n",
    "6. `Parameters` are the weights and biases internal to the model. `Hyperparameters` are the nerd knobs that can be turned to change how the model is trained; per the textbook-- things like training rate and epochs.\n",
    "\n",
    "7. a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a778be8-c0d2-411d-8266-f28a692c8946",
   "metadata": {},
   "source": [
    "## 2. [12 pts] Pick the Iris dataset of the Scikit-learn datasets for classification which is included in the library (i.e. the dataset can be loaded with datasets.load_) and find out the following:\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb564d44-48bc-4a63-848b-a28764dfdb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1)\tNumber of items in the dataset: 150\n",
      "\n",
      "\n",
      "2)\tFeature names and types:\n",
      "sepal length (cm)    float64\n",
      "sepal width (cm)     float64\n",
      "petal length (cm)    float64\n",
      "petal width (cm)     float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "3)\tThere are 3 category with names names ['setosa' 'versicolor' 'virginica'], and types int64\n",
      "\n",
      "\n",
      "number of categories: 3\n",
      "\n",
      "\n",
      " First feature \"sepal length (cm)\" mode: 0    5.0\n",
      "Name: sepal length (cm), dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "print(f'\\n\\n1)\\tNumber of items in the dataset: {iris.data.shape[0]}')\n",
    "# print(f'\\n\\nFeature names: {iris.feature_names}')\n",
    "print(f'\\n\\n2)\\tFeature names and types:\\n{iris.data.dtypes}')\n",
    "print(f'\\n\\n3)\\tThere are {len(iris.target.unique())} categories with names names {iris.target_names}, and types {iris.target.dtype}')\n",
    "print(f'\\n\\nnumber of categories: {len(iris.target.unique())}')\n",
    "first_two_features = iris.data.columns[:2]\n",
    "print(f'\\n\\n First feature \"{first_two_features[0]}\" mode: {iris.data[first_two_features[0]].mode()}')\n",
    "print('\\n\\n')\n",
    "print(iris.keys())\n",
    "print(iris.data.info(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79f6cc-6f1e-4e39-9c69-de935c0df9f9",
   "metadata": {},
   "source": [
    "## 3. [8 pts] Next, locate the Wine dataset, load it, and explore it to find out the same characteristics (as in #2.i-iv) once more.\n",
    "  1. the number of data points\n",
    "  2. the number of features and their types\n",
    "  3. the number and name of categories (i.e. the target field)\n",
    "  4. the mean (or mode if nominal) of the first two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17560d27-05d6-4063-a572-7ffcd2a72ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "wines = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373b322-4cff-4db3-afec-c3efaf53bdb1",
   "metadata": {},
   "source": [
    "## 4. [20 pts] Use the following lines of code to display feature pairs from the Iris dataset:\n",
    "```\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "iris = sklearn.datasets.load_iris()\n",
    "iris_df = pd.DataFrame(\n",
    "data= np.c_[iris.data, [iris.target_names[v] for v in iris.target]],\n",
    "columns= iris.feature_names + ['species'])\n",
    "cols = iris_df.columns.drop('species')\n",
    "iris_df[cols] = iris_df[cols].apply(pd.to_numeric)\n",
    "g = sns.pairplot(iris_df, hue='species')\n",
    "```\n",
    "From the plots, which feature(s) shows the most promising separation power for machine\n",
    "learning?\n",
    "Now plot the features of the Wine dataset in question 2. When there are too many features,\n",
    "it is possible to switch the dataset or update your code (pandas Dataframe line) to look at\n",
    "only a certain number of features at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f5d97-fad8-433c-b4f8-acb6a29fc9ed",
   "metadata": {},
   "source": [
    "## 5. [20 pts] Consider the Iris dataset, refer to the plots in the previous question, and discuss/think/outline an unsupervised approach to group the dataset into non-overlapping clusters. Answer the following questions:\n",
    "  1. Which features would you use?\n",
    "  2. Are three clusters obvious from the plots?\n",
    "  3. What about four clusters? Either roughly mark them manually (i.e. indicating cutoff thresholds) on a few plots if possible, or specify their ranges (i.e. provide the range of values per cluster per each feature youâ€™ve chosen to include).\n",
    "  4. For this problem, is there any relation between classification and clustering since the labels are already given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899e3d6-f2d2-4bc1-9f26-7cb621e4e9c5",
   "metadata": {},
   "source": [
    "## 6. [20 pts] Using the scikit-learn class descriptions for Naive Bayes and decision trees, classify the Iris dataset from question 4. Your code should be very similar to that in the Module 1 Jupyter notebook. Make sure to divide the dataset into two portions, one for training and the other for testing, as is done in that notebook, using sklearn.model_selection.train_test_split (and make sure that shuffle is on). Then, answer the following questions:\n",
    "  1. Why is using shuffle important for this dataset?\n",
    "  2. Which classifier has the higher performance?\n",
    "  3. Does more training help? Test this by increasing the training dataset size from let's say 1% training (with the remaining 99% testing), and then 5% training, 10% training, etc.\n",
    "  4. Will the performance plateau? Show it on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4eb41-e454-4895-9294-0e5f5e005bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
