{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff456234-feec-4311-baaa-224629c93f4b",
   "metadata": {},
   "source": [
    "# Overview\n",
    "From the Kaggle web site (https://www.kaggle.com/datasets) download the Suicide Rates Overview 1985 to 2016 dataset. This dataset has 12 features and 27820 data points. In this assignment we would like to develop a machine learned model to predict, given some feature vectors, if the outcome would be suicide or not, as a binary dependent variable. The binary categories could be {\"low suicide rate\", \"high suicide rate\"}. (Note that a different approach could seek to generate a numerical value by solving a regression problem.)\n",
    "\n",
    "\n",
    "A machine learning solution would require us to pre-process the dataset and prepare/design our experimentation.\n",
    "\n",
    "\n",
    "Load the dataset in your model development framework (Jupyter notebook) and examine the features. Note that the Kaggle website also has histograms that you can inspect. However, you might want to look at the data grouped by some other features. For example, what does the 'number of suicides / 100k' histogram look like from country to country?\n",
    "\n",
    "\n",
    "To answer the following questions, you have to think thoroughly, and possibly attempt some pilot experiments. There is no one right or wrong answer to some questions below, but you will always need to work from the data to build a convincing argument for your audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27347717-2e58-4db1-ad9c-5ae5925105ee",
   "metadata": {},
   "source": [
    "### 1. [10 pts] Due to the severity of this real-world crisis, what information would be the most important to \"machine learn\"? Can it be learned? (Note that this is asking you to define the big-picture question that we want to answer from this dataset. This is not asking you to conjecture which feature is going to turn out being important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bff51-9537-404b-a611-7ef27ed5dba3",
   "metadata": {},
   "source": [
    "#### 1. Answer\n",
    "\n",
    "In my opinion the most important thing to determine with the dataset is what causes suicides.  It seems a problem people struggle to understand, therefore teaching a machine to understand it wouldn't be possible.  Maybe we could teach a machine to observe for causes of suicide.  I think a machine can be taught to observe for causes of suicide, and how likely they are to occur in a population or an individual.  It seems unlikely that we will be able to do that with this dataset.  The data has too few features-- `year` and `generation` are tightly correlated, as are `year` and `country-year`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a828cad-7e4b-4073-92f0-b85456a81b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows=27820 #columns=12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>suicides_no</th>\n",
       "      <th>population</th>\n",
       "      <th>suicides/100k pop</th>\n",
       "      <th>country-year</th>\n",
       "      <th>HDI for year</th>\n",
       "      <th>gdp_for_year ($)</th>\n",
       "      <th>gdp_per_capita ($)</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>21</td>\n",
       "      <td>312900</td>\n",
       "      <td>6.71</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>35-54 years</td>\n",
       "      <td>16</td>\n",
       "      <td>308000</td>\n",
       "      <td>5.19</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Silent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>female</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>14</td>\n",
       "      <td>289700</td>\n",
       "      <td>4.83</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>75+ years</td>\n",
       "      <td>1</td>\n",
       "      <td>21800</td>\n",
       "      <td>4.59</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>G.I. Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34 years</td>\n",
       "      <td>9</td>\n",
       "      <td>274300</td>\n",
       "      <td>3.28</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Boomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year     sex          age  suicides_no  population  \\\n",
       "0  Albania  1987    male  15-24 years           21      312900   \n",
       "1  Albania  1987    male  35-54 years           16      308000   \n",
       "2  Albania  1987  female  15-24 years           14      289700   \n",
       "3  Albania  1987    male    75+ years            1       21800   \n",
       "4  Albania  1987    male  25-34 years            9      274300   \n",
       "\n",
       "   suicides/100k pop country-year  HDI for year   gdp_for_year ($)   \\\n",
       "0               6.71  Albania1987           NaN          2156624900   \n",
       "1               5.19  Albania1987           NaN          2156624900   \n",
       "2               4.83  Albania1987           NaN          2156624900   \n",
       "3               4.59  Albania1987           NaN          2156624900   \n",
       "4               3.28  Albania1987           NaN          2156624900   \n",
       "\n",
       "   gdp_per_capita ($)       generation  \n",
       "0                 796     Generation X  \n",
       "1                 796           Silent  \n",
       "2                 796     Generation X  \n",
       "3                 796  G.I. Generation  \n",
       "4                 796          Boomers  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Experiments\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualizations\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('./datasets/master.csv', thousands=',')\n",
    "\n",
    "# Sanity\n",
    "print(f'#rows={len(df)} #columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275dcc1-6f43-4ca1-895d-a38736eba543",
   "metadata": {},
   "source": [
    "### 2. [10 pts] Explain in detail how one should set up the problem. Would it be a regression or a classification problem? Is any unsupervised approach, to look for patterns, worthwhile?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b4d97-e0c6-4d7a-be09-da17ee4360ab",
   "metadata": {},
   "source": [
    "#### 2. Answer\n",
    "Starting with the last question, \"is any unsupervised approach worthwhile,\" considering this dataset no.  The dataset has labeled features.  It might be useful do a comparison of omitting labels and testing whether an unsupervised approach finds possible relations.\n",
    "\n",
    "To address the larger question-- we could use a Decision Tree, or by extension a random forest; but this is also a regression problem since we want to determine where on a numeric scale the rate of suicides per 100,000 persons will trend.  To setup the problem we will need to normalize the numeric features `HDI for year`, `gdp_for_year($)`, `gdp_per_capita ($)`, `population`, `suicides_no`, `year` (we could have standardized them too); map the ordinal features `age` and `generation`; and encode the nominal features `country` and `sex`.  \n",
    "\n",
    "We will drop the feature `country-year` since it is captured in the dataset.  We could have chosen to drop `country` and `year` however I like that year is numeric, and perhaps there are relations between the year and different countries.  Additionally, I will train two version of the model, one including `suicides_no` and `population` and one without.  These two features in combination have a correlation coefficient of 1 with the target.  That doesn't answer the question _I_ am curious about, which is about what in a population makes them susceptible to suicide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f517f-d25b-40bb-86b9-c87eeb3f03ef",
   "metadata": {},
   "source": [
    "### 3. [20 pts] What should be the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16329-165f-45bb-bac2-ec630adfdfcf",
   "metadata": {},
   "source": [
    "#### 3. Answer\n",
    "According to the dataset page on Kaggle, the dataset intends to collate information on \"suicide rates by cohort.\"  This means the `suicides/100k pop` is the target, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3b8e6-cc9a-484f-84f8-faed73f85dbe",
   "metadata": {},
   "source": [
    "\n",
    "### 4. [20 pts] Find some strong correlations between the independent variables and the dependent variable you decided and use them to rank the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f42691-5d0f-472d-b839-727c43ba69c4",
   "metadata": {},
   "source": [
    "#### 4. Answer\n",
    "\n",
    "Finding correlations when there are missing or NaN values would be in error since it would skew the data.  Granted, I will skew the data by adding the mean value if the feature is a numeric type.  After processing the data I will use Pandas `DataFrame.corrwith` to find correlation with the target column `suicides/100k pop`, which I will rename to `suicides per 100k pop`.  \n",
    "\n",
    "\n",
    "Here is a summary of the steps I took.\n",
    "- Drop `country-year` since it is accounted for with other features.\n",
    "- Remove ($), parens and `/` from feature names. Note `gdp_for_year ($)` gave me trouble so I had to do it in an irritating way. \n",
    "- Check for duplicates. _None found._\n",
    "- Check for null values. _None found._\n",
    "- Check for NaN. _Only found NaN values in the `HDI for year` features. `DataFrame.describe() shows 8364 non-NaN values, which means almost more than 2x that number are NaN.  Perhaps I should drop this features, but there are so few already that I am choosing to keep it._\n",
    "- Fill NaN values of `HDI for year`with the mean of `HDI for year`.\n",
    "- Scale the numeric features using normalization\n",
    "- Use Pandas to check for correlation of each feature with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6222166b-00b4-41ff-92b6-472b83bf3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country                object\n",
      "year                    int64\n",
      "sex                    object\n",
      "age                    object\n",
      "suicides_no             int64\n",
      "population              int64\n",
      "suicides/100k pop     float64\n",
      "country-year           object\n",
      "HDI for year          float64\n",
      " gdp_for_year ($)       int64\n",
      "gdp_per_capita ($)      int64\n",
      "generation             object\n",
      "dtype: object\n",
      "gdp_for_year\n",
      "Index(['country', 'year', 'sex', 'age', 'population', 'suicides per 100k pop',\n",
      "       'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'],\n",
      "      dtype='object')\n",
      "Count of duplicates: 27820\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "population               False\n",
      "suicides per 100k pop    False\n",
      "HDI for year              True\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "See that there is only one column with NaN values:\n",
      "country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "population               False\n",
      "suicides per 100k pop    False\n",
      "HDI for year              True\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\"HDI for year\" is the only column with NaN.  Let's compare how many NaN 27820 vs 27820 non-NaN of a total of 27820 samples\n",
      "count    8364.000000\n",
      "mean        0.776601\n",
      "std         0.093367\n",
      "min         0.483000\n",
      "25%         0.713000\n",
      "50%         0.779000\n",
      "75%         0.855000\n",
      "max         0.944000\n",
      "Name: HDI for year, dtype: float64\n",
      "This is reporting specious results.  DataFrame.describe() is showing 8364 non-NaN values.  Let's fill NaN with the means and see what that changes.\n",
      "\n",
      "\n",
      "See, no more missing values.country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "population               False\n",
      "suicides per 100k pop    False\n",
      "HDI for year             False\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "country ['Albania' 'Antigua and Barbuda' 'Argentina' 'Armenia' 'Aruba' 'Australia'\n",
      " 'Austria' 'Azerbaijan' 'Bahamas' 'Bahrain' 'Barbados' 'Belarus' 'Belgium'\n",
      " 'Belize' 'Bosnia and Herzegovina' 'Brazil' 'Bulgaria' 'Cabo Verde'\n",
      " 'Canada' 'Chile' 'Colombia' 'Costa Rica' 'Croatia' 'Cuba' 'Cyprus'\n",
      " 'Czech Republic' 'Denmark' 'Dominica' 'Ecuador' 'El Salvador' 'Estonia'\n",
      " 'Fiji' 'Finland' 'France' 'Georgia' 'Germany' 'Greece' 'Grenada'\n",
      " 'Guatemala' 'Guyana' 'Hungary' 'Iceland' 'Ireland' 'Israel' 'Italy'\n",
      " 'Jamaica' 'Japan' 'Kazakhstan' 'Kiribati' 'Kuwait' 'Kyrgyzstan' 'Latvia'\n",
      " 'Lithuania' 'Luxembourg' 'Macau' 'Maldives' 'Malta' 'Mauritius' 'Mexico'\n",
      " 'Mongolia' 'Montenegro' 'Netherlands' 'New Zealand' 'Nicaragua' 'Norway'\n",
      " 'Oman' 'Panama' 'Paraguay' 'Philippines' 'Poland' 'Portugal'\n",
      " 'Puerto Rico' 'Qatar' 'Republic of Korea' 'Romania' 'Russian Federation'\n",
      " 'Saint Kitts and Nevis' 'Saint Lucia' 'Saint Vincent and Grenadines'\n",
      " 'San Marino' 'Serbia' 'Seychelles' 'Singapore' 'Slovakia' 'Slovenia'\n",
      " 'South Africa' 'Spain' 'Sri Lanka' 'Suriname' 'Sweden' 'Switzerland'\n",
      " 'Thailand' 'Trinidad and Tobago' 'Turkey' 'Turkmenistan' 'Ukraine'\n",
      " 'United Arab Emirates' 'United Kingdom' 'United States' 'Uruguay'\n",
      " 'Uzbekistan']\n",
      "sex ['male' 'female']\n",
      "age ['15-24 years' '35-54 years' '75+ years' '25-34 years' '55-74 years'\n",
      " '5-14 years']\n",
      "generation ['Generation X' 'Silent' 'G.I. Generation' 'Boomers' 'Millenials'\n",
      " 'Generation Z']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "['country', 'year', 'sex', 'age', 'population', 'suicides per 100k pop', 'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'] \n",
      " ['country', 'year', 'sex', 'age', 'population', 'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'] \n",
      " suicides per 100k pop \n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Pandas DataFrame.corrwith():\n",
      "--------------------------------------------------\n",
      "generation              -0.049820\n",
      "year                    -0.039037\n",
      "gdp_per_capita           0.001785\n",
      "population               0.008285\n",
      "gdp_for_year             0.025240\n",
      "HDI for year             0.037290\n",
      "country                  0.055224\n",
      "age                      0.187215\n",
      "sex                      0.391496\n",
      "suicides per 100k pop    1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def fix_data_before_numeric_or_ordinal_changes(df, printer=True):\n",
    "    if printer: print(df.dtypes)\n",
    "    \n",
    "    # df = df.drop(columns=['country-year', 'population', 'suicides_no'])\n",
    "    df = df.drop(columns=['suicides_no', 'country-year'])\n",
    "    df = df.rename(columns={'gdp_per_capita ($)': 'gdp_per_capita', 'suicides/100k pop': 'suicides per 100k pop'})\n",
    "    \n",
    "    \n",
    "    for c in df.columns:\n",
    "        if 'gdp_for_year' in c:\n",
    "            new_column_name = c.replace('($)','').strip()\n",
    "            df = df.rename(columns={c: new_column_name})\n",
    "            if printer: print(new_column_name)\n",
    "    \n",
    "    if printer: print(df.columns)\n",
    "    \n",
    "    # Check for duplicates, this adds a new column to the dataset\n",
    "    if printer: print(f'Count of duplicates: {len(df.duplicated())}')\n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    if printer: print(df.isnull().any())\n",
    "    \n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    if printer: print(f'See that there is only one column with NaN values:\\n{df.isna().any()}')\n",
    "    if printer: print(f'\"HDI for year\" is the only column with NaN.  Let\\'s compare how many NaN {df[\"HDI for year\"].isna().count()} vs {df[\"HDI for year\"].notna().count()} non-NaN of a total of {len(df[\"HDI for year\"])} samples')\n",
    "    if printer: print(df[\"HDI for year\"].describe())\n",
    "    if printer: print(f'This is reporting specious results.  DataFrame.describe() is showing 8364 non-NaN values.  Let\\'s fill NaN with the means and see what that changes.')\n",
    "    \n",
    "    # Replace NaN values, or leave it as is otherwise\n",
    "    mean_value = df['HDI for year'].mean()\n",
    "    df['HDI for year'] = df['HDI for year'].fillna(mean_value)\n",
    "    # This shows that there are no longer any missing values\n",
    "    if printer: print(f'\\n\\nSee, no more missing values.{df.isna().any()}')\n",
    "    \n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    ## Using the method described in the module notebook, check unique values by column\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            if printer: print(col, df[col].unique())\n",
    "    return df\n",
    "\n",
    "\n",
    "df4 = fix_data_before_numeric_or_ordinal_changes(df)\n",
    "\n",
    "# Scale the numeric features\n",
    "numeric_columns = list(df4.select_dtypes(exclude=['object']).columns)\n",
    "sc = StandardScaler()\n",
    "df4[numeric_columns] = sc.fit_transform(df4[numeric_columns])\n",
    "\n",
    "\n",
    "## LabelEncoder\n",
    "# Encode object types. They are all strings.  Save the labelencoders paired with the column names so we can reverse the values later\n",
    "columns_to_encode = df4.select_dtypes(include='object')\n",
    "for column in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df4[column] = le.fit_transform(df4[column].astype(str))\n",
    "\n",
    "\n",
    "# Use pandas to print correlation\n",
    "print('\\n\\n\\n')\n",
    "labels = list(df4.columns)\n",
    "feature_labels = list(df4.columns)\n",
    "target_label = 'suicides per 100k pop'\n",
    "feature_labels.remove(target_label)\n",
    "X = df4.drop(target_label, axis=1).values\n",
    "y = df4[target_label].values\n",
    "print(labels, '\\n',feature_labels, '\\n', target_label, '\\n', )\n",
    "print(f'\\n\\n{\"-\"*50}\\nPandas DataFrame.corrwith():\\n{\"-\"*50}\\n{df4.corrwith(df4[target_label]).sort_values()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987d691-2268-4e51-b85f-8a71c19362dc",
   "metadata": {},
   "source": [
    "### 5. [20 pts] Pre-process the dataset and list the major features you want to use. Note that not all features are crucial. For example, country-year variable is a derived feature and for a classifier it would not be necessary to include the year, the country and the country -year together. In fact, one must avoid adding a derived feature and the original at the same time.\n",
    "List the independent features you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fbabc-0f85-458d-9509-039e2e39edf7",
   "metadata": {},
   "source": [
    "#### 5. Answer\n",
    "My independent variables will be `age`, `country`, `generation`, `gdp_per_capita`, `gdp_for_year`, `HDI for year`, `population`, `sex`, and `year`.  I will not be using `suicide_no`because dividing it with `population` is equal to `suicides per 100k pop`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75406b89-4755-423e-a31c-cf92bbb7b3e5",
   "metadata": {},
   "source": [
    "### 6. [20 pts] Devise a classification problem and present a working prototype model. (It does not have to perform great, but it has to be functional.) Note that we will continue with this problem in the following modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1d1b6-b25a-4e98-b5d1-d88bdf6c50f2",
   "metadata": {},
   "source": [
    "#### 6. Answer\n",
    "\n",
    "I called this problem a regression problem at the start of the homework due to the datatype I saw in `suicides/100k pop`.  I see now that it is a classification problem, where the classes are bins.  \n",
    "\n",
    "\n",
    "For the sake of my earlier statements I tried to get good results with a regressor. The `GradientBoostingRegressor` has good documentation, many examples, and plenty of hyperparameters to tune.  I tried it with the default parameters and acheived an MSE of 42, making this barely better than a flip of the coin.  I tried normalizing features, rather than standardizing them, which received worse scores in MSE and R^2.  Note: R^2 is the \"coefficient of determination;\" where a score of 1 is perfect, a score of 0 means the input features aren't used; and a negative score means you have a bad model. Finally, I tried using 5x more trees than the default, and lowering the learning rate to 10% of the default; the results are better than normalizing the features and worse than using the default parameters...\n",
    "\n",
    "\n",
    "To fall on my sword, I lastly show a `RandomForestClassifier`.  Using the default parameters it acheives 93% accuracy and an F-1 score of 93%.  This is excellent, given that I tried to force a wrong option for an entire day.\n",
    "\n",
    "\n",
    "Recall that in `#4` I extracted `X` and `y` for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d490237c-32c8-4745-91b5-b3f7431867d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Test with Gradient Boosted Regressor\n",
      "******* standardized features\n",
      "Mean Squared Error: 0.42\n",
      "R^2 Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def q6_gbr_scaled(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gbr.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'******* Test with Gradient Boosted Regressor\\n******* standardized features')\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "    print(f'R^2 Score: {r2:.2f}')\n",
    "q6_gbr_scaled(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e4d37e-45ae-4d4f-b9bd-6a8c704e597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Test with Gradient Boosted Regressor\n",
      "******* normalized features\n",
      "Mean Squared Error: 0.48\n",
      "R^2 Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def q6_gbr_normalized(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    normer = Normalizer()\n",
    "    X_train = normer.fit_transform(X_train)\n",
    "    X_test = normer.transform(X_test)\n",
    "    \n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gbr.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'******* Test with Gradient Boosted Regressor\\n******* normalized features')\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "    print(f'R^2 Score: {r2:.2f}')\n",
    "q6_gbr_normalized(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657cca1b-5440-431e-8174-83daf3bcb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Test with Gradient Boosted Regressor\n",
      "******* custom parameters\n",
      "Mean Squared Error: 0.44\n",
      "R^2 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def q6_gbr_custom_params(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Custom parameters. Similar to what examples on Scitkit learn have for GradientBoostedRegressor\n",
    "    params = {\n",
    "        \"n_estimators\": 500,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "    }\n",
    "    \n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'******* Test with Gradient Boosted Regressor\\n******* custom parameters')\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "    print(f'R^2 Score: {r2:.2f}')\n",
    "\n",
    "q6_gbr_custom_params(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec0202f-7132-4617-b036-f5fbe4018ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with Random Forest Classifier\n",
      "default parameters\n",
      "Accuracy: 0.93\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      2819\n",
      "           1       0.93      0.93      0.93      2745\n",
      "\n",
      "    accuracy                           0.93      5564\n",
      "   macro avg       0.93      0.93      0.93      5564\n",
      "weighted avg       0.93      0.93      0.93      5564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def q6_rfc(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f'Test with Random Forest Classifier\\ndefault parameters')\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Classification Report:\\n', report)\n",
    "\n",
    "# Re-work for continuous data\n",
    "df_rfc = fix_data_before_numeric_or_ordinal_changes(df, printer=False)\n",
    "\n",
    "columns_to_encode = df_rfc.select_dtypes(include='object')\n",
    "for column in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df_rfc[column] = le.fit_transform(df_rfc[column].astype(str))\n",
    "X = df_rfc.drop(columns=['suicides per 100k pop'])\n",
    "y = (df_rfc['suicides per 100k pop'] > df_rfc['suicides per 100k pop'].median()).astype(int)  \n",
    "q6_rfc(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771cb1b-6b4a-40d1-87af-82740e585329",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Raschka, Sebastian, et al. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022.\n",
    "2. Guven, Erhan. Applied Machine Learning: Module 3 Notebook.  Last accessed 6 February, 2025.\n",
    "3. Scikit-Learn. https://scikit-learn.org/stable/. Last accessed 6 February, 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
