{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff456234-feec-4311-baaa-224629c93f4b",
   "metadata": {},
   "source": [
    "# Overview\n",
    "From the Kaggle web site (https://www.kaggle.com/datasets) download the Suicide Rates Overview 1985 to 2016 dataset. This dataset has 12 features and 27820 data points. In this assignment we would like to develop a machine learned model to predict, given some feature vectors, if the outcome would be suicide or not, as a binary dependent variable. The binary categories could be {\"low suicide rate\", \"high suicide rate\"}. (Note that a different approach could seek to generate a numerical value by solving a regression problem.)\n",
    "\n",
    "\n",
    "A machine learning solution would require us to pre-process the dataset and prepare/design our experimentation.\n",
    "\n",
    "\n",
    "Load the dataset in your model development framework (Jupyter notebook) and examine the features. Note that the Kaggle website also has histograms that you can inspect. However, you might want to look at the data grouped by some other features. For example, what does the 'number of suicides / 100k' histogram look like from country to country?\n",
    "\n",
    "\n",
    "To answer the following questions, you have to think thoroughly, and possibly attempt some pilot experiments. There is no one right or wrong answer to some questions below, but you will always need to work from the data to build a convincing argument for your audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27347717-2e58-4db1-ad9c-5ae5925105ee",
   "metadata": {},
   "source": [
    "### 1. [10 pts] Due to the severity of this real-world crisis, what information would be the most important to \"machine learn\"? Can it be learned? (Note that this is asking you to define the big-picture question that we want to answer from this dataset. This is not asking you to conjecture which feature is going to turn out being important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bff51-9537-404b-a611-7ef27ed5dba3",
   "metadata": {},
   "source": [
    "#### 1. Answer\n",
    "\n",
    "In my opinion the most important thing to determine with the dataset is what causes suicides.  It seems a problem people struggle to understand, therefore teaching a machine to understand it wouldn't be possible.  Maybe we could teach a machine to observe for causes of suicide.  I think a machine can be taught to observe for causes of suicide, and how likely they are to occur in a population or an individual.  It seems unlikely that we will be able to do that with this dataset.  The data has too few features-- `year` and `generation` are tightly correlated, as are `year` and `country-year`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a828cad-7e4b-4073-92f0-b85456a81b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows=27820 #columns=12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>suicides_no</th>\n",
       "      <th>population</th>\n",
       "      <th>suicides/100k pop</th>\n",
       "      <th>country-year</th>\n",
       "      <th>HDI for year</th>\n",
       "      <th>gdp_for_year ($)</th>\n",
       "      <th>gdp_per_capita ($)</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>21</td>\n",
       "      <td>312900</td>\n",
       "      <td>6.71</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>35-54 years</td>\n",
       "      <td>16</td>\n",
       "      <td>308000</td>\n",
       "      <td>5.19</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Silent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>female</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>14</td>\n",
       "      <td>289700</td>\n",
       "      <td>4.83</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>75+ years</td>\n",
       "      <td>1</td>\n",
       "      <td>21800</td>\n",
       "      <td>4.59</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>G.I. Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34 years</td>\n",
       "      <td>9</td>\n",
       "      <td>274300</td>\n",
       "      <td>3.28</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156624900</td>\n",
       "      <td>796</td>\n",
       "      <td>Boomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year     sex          age  suicides_no  population  \\\n",
       "0  Albania  1987    male  15-24 years           21      312900   \n",
       "1  Albania  1987    male  35-54 years           16      308000   \n",
       "2  Albania  1987  female  15-24 years           14      289700   \n",
       "3  Albania  1987    male    75+ years            1       21800   \n",
       "4  Albania  1987    male  25-34 years            9      274300   \n",
       "\n",
       "   suicides/100k pop country-year  HDI for year   gdp_for_year ($)   \\\n",
       "0               6.71  Albania1987           NaN          2156624900   \n",
       "1               5.19  Albania1987           NaN          2156624900   \n",
       "2               4.83  Albania1987           NaN          2156624900   \n",
       "3               4.59  Albania1987           NaN          2156624900   \n",
       "4               3.28  Albania1987           NaN          2156624900   \n",
       "\n",
       "   gdp_per_capita ($)       generation  \n",
       "0                 796     Generation X  \n",
       "1                 796           Silent  \n",
       "2                 796     Generation X  \n",
       "3                 796  G.I. Generation  \n",
       "4                 796          Boomers  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Experiments\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualizations\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('./datasets/master.csv', thousands=',')\n",
    "\n",
    "# Sanity\n",
    "print(f'#rows={len(df)} #columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275dcc1-6f43-4ca1-895d-a38736eba543",
   "metadata": {},
   "source": [
    "### 2. [10 pts] Explain in detail how one should set up the problem. Would it be a regression or a classification problem? Is any unsupervised approach, to look for patterns, worthwhile?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b4d97-e0c6-4d7a-be09-da17ee4360ab",
   "metadata": {},
   "source": [
    "#### 2. Answer\n",
    "Starting with the last question, \"is any unsupervised approach worthwhile,\" considering this dataset no.  The dataset has labeled features.  It might be useful do a comparison of omitting labels and testing whether an unsupervised approach finds possible relations.\n",
    "\n",
    "To address the larger question-- we could use a Decision Tree, or by extension a random forest; but this is also a regression problem since we want to determine where on a numeric scale the rate of suicides per 100,000 persons will trend.  To setup the problem we will need to normalize the numeric features `HDI for year`, `gdp_for_year($)`, `gdp_per_capita ($)`, `population`, `suicides_no`, `year` (we could have standardized them too); map the ordinal features `age` and `generation`; and encode the nominal features `country` and `sex`.  \n",
    "\n",
    "We will drop the feature `country-year` since it is captured in the dataset.  We could have chosen to drop `country` and `year` however I like that year is numeric, and perhaps there are relations between the year and different countries.  Additionally, I will train two version of the model, one including `suicides_no` and `population` and one without.  These two features in combination have a correlation coefficient of 1 with the target.  That doesn't answer the question _I_ am curious about, which is about what in a population makes them susceptible to suicide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f517f-d25b-40bb-86b9-c87eeb3f03ef",
   "metadata": {},
   "source": [
    "### 3. [20 pts] What should be the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16329-165f-45bb-bac2-ec630adfdfcf",
   "metadata": {},
   "source": [
    "#### 3. Answer\n",
    "According to the dataset page on Kaggle, the dataset intends to collate information on \"suicide rates by cohort.\"  This means the `suicides/100k pop` is the target, which makes sense.  We will explore it in the next question, but that may mean we can remove `suicides_no` or `population` features for training deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3b8e6-cc9a-484f-84f8-faed73f85dbe",
   "metadata": {},
   "source": [
    "\n",
    "### 4. [20 pts] Find some strong correlations between the independent variables and the dependent variable you decided and use them to rank the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f42691-5d0f-472d-b839-727c43ba69c4",
   "metadata": {},
   "source": [
    "#### 4. Answer\n",
    "\n",
    "Finding correlations when there are missing or NaN values would be in error since it would skew the data.  Granted, I will skew the data by adding the mean value if the feature is a numeric type.  After processing the data I will use Pandas `DataFrame.corrwith` to find correlation with the target column `suicides/100k pop`, which I will rename to `suicides per 100k pop`.  \n",
    "\n",
    "\n",
    "Here is a summary of the steps I took.\n",
    "- Drop `country-year` since it is accounted for with other features.\n",
    "- Remove ($), parens and `/` from feature names. Note `gdp_for_year ($)` gave me trouble so I had to do it in an irritating way. \n",
    "- Check for duplicates. _None found._\n",
    "- Check for null values. _None found._\n",
    "- Check for NaN. _Only found NaN values in the `HDI for year` features. `DataFrame.describe() shows 8364 non-NaN values, which means almost more than 2x that number are NaN.  Perhaps I should drop this features, but there are so few already that I am choosing to keep it._\n",
    "- Fill NaN values of `HDI for year`with the mean of `HDI for year`.\n",
    "- Scale the numeric features using normalization\n",
    "- Use Pandas to check for correlation of each feature with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6222166b-00b4-41ff-92b6-472b83bf3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country                object\n",
      "year                    int64\n",
      "sex                    object\n",
      "age                    object\n",
      "suicides_no             int64\n",
      "population              int64\n",
      "suicides/100k pop     float64\n",
      "country-year           object\n",
      "HDI for year          float64\n",
      " gdp_for_year ($)       int64\n",
      "gdp_per_capita ($)      int64\n",
      "generation             object\n",
      "dtype: object\n",
      "gdp_for_year\n",
      "Index(['country', 'year', 'sex', 'age', 'suicides per 100k pop',\n",
      "       'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'],\n",
      "      dtype='object')\n",
      "Count of duplicates: 27820\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "suicides per 100k pop    False\n",
      "HDI for year              True\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "See that there is only one column with NaN values: country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "suicides per 100k pop    False\n",
      "HDI for year              True\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\"HDI for year\" is the only column with NaN.  Let's compare how many NaN 27820 vs 27820 non-NaN of a total of 27820 samples\n",
      "count    8364.000000\n",
      "mean        0.776601\n",
      "std         0.093367\n",
      "min         0.483000\n",
      "25%         0.713000\n",
      "50%         0.779000\n",
      "75%         0.855000\n",
      "max         0.944000\n",
      "Name: HDI for year, dtype: float64\n",
      "This is reporting specious results.  DataFrame.describe() is showing 8364 non-NaN values.  Let's fill NaN with the means and see what that changes.\n",
      "\n",
      "\n",
      "See, no more missing values.country                  False\n",
      "year                     False\n",
      "sex                      False\n",
      "age                      False\n",
      "suicides per 100k pop    False\n",
      "HDI for year             False\n",
      "gdp_for_year             False\n",
      "gdp_per_capita           False\n",
      "generation               False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "country ['Albania' 'Antigua and Barbuda' 'Argentina' 'Armenia' 'Aruba' 'Australia'\n",
      " 'Austria' 'Azerbaijan' 'Bahamas' 'Bahrain' 'Barbados' 'Belarus' 'Belgium'\n",
      " 'Belize' 'Bosnia and Herzegovina' 'Brazil' 'Bulgaria' 'Cabo Verde'\n",
      " 'Canada' 'Chile' 'Colombia' 'Costa Rica' 'Croatia' 'Cuba' 'Cyprus'\n",
      " 'Czech Republic' 'Denmark' 'Dominica' 'Ecuador' 'El Salvador' 'Estonia'\n",
      " 'Fiji' 'Finland' 'France' 'Georgia' 'Germany' 'Greece' 'Grenada'\n",
      " 'Guatemala' 'Guyana' 'Hungary' 'Iceland' 'Ireland' 'Israel' 'Italy'\n",
      " 'Jamaica' 'Japan' 'Kazakhstan' 'Kiribati' 'Kuwait' 'Kyrgyzstan' 'Latvia'\n",
      " 'Lithuania' 'Luxembourg' 'Macau' 'Maldives' 'Malta' 'Mauritius' 'Mexico'\n",
      " 'Mongolia' 'Montenegro' 'Netherlands' 'New Zealand' 'Nicaragua' 'Norway'\n",
      " 'Oman' 'Panama' 'Paraguay' 'Philippines' 'Poland' 'Portugal'\n",
      " 'Puerto Rico' 'Qatar' 'Republic of Korea' 'Romania' 'Russian Federation'\n",
      " 'Saint Kitts and Nevis' 'Saint Lucia' 'Saint Vincent and Grenadines'\n",
      " 'San Marino' 'Serbia' 'Seychelles' 'Singapore' 'Slovakia' 'Slovenia'\n",
      " 'South Africa' 'Spain' 'Sri Lanka' 'Suriname' 'Sweden' 'Switzerland'\n",
      " 'Thailand' 'Trinidad and Tobago' 'Turkey' 'Turkmenistan' 'Ukraine'\n",
      " 'United Arab Emirates' 'United Kingdom' 'United States' 'Uruguay'\n",
      " 'Uzbekistan']\n",
      "sex ['male' 'female']\n",
      "age ['15-24 years' '35-54 years' '75+ years' '25-34 years' '55-74 years'\n",
      " '5-14 years']\n",
      "generation ['Generation X' 'Silent' 'G.I. Generation' 'Boomers' 'Millenials'\n",
      " 'Generation Z']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "['country', 'year', 'sex', 'age', 'suicides per 100k pop', 'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'] \n",
      " ['country', 'year', 'sex', 'age', 'HDI for year', 'gdp_for_year', 'gdp_per_capita', 'generation'] \n",
      " suicides per 100k pop \n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Pandas DataFrame.corrwith():\n",
      "--------------------------------------------------\n",
      "generation              -0.049820\n",
      "year                    -0.039037\n",
      "gdp_per_capita           0.001785\n",
      "gdp_for_year             0.025240\n",
      "HDI for year             0.037290\n",
      "country                  0.055224\n",
      "age                      0.187215\n",
      "sex                      0.391496\n",
      "suicides per 100k pop    1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def fix_data_before_numeric_or_ordinal_changes(df, printer=True):\n",
    "    if printer: print(df.dtypes)\n",
    "    \n",
    "    df = df.drop(columns=['country-year', 'population', 'suicides_no'])\n",
    "    df = df.rename(columns={'gdp_per_capita ($)': 'gdp_per_capita', 'suicides/100k pop': 'suicides per 100k pop'})\n",
    "    \n",
    "    \n",
    "    for c in df.columns:\n",
    "        if 'gdp_for_year' in c:\n",
    "            new_column_name = c.replace('($)','').strip()\n",
    "            df = df.rename(columns={c: new_column_name})\n",
    "            if printer: print(new_column_name)\n",
    "    \n",
    "    if printer: print(df.columns)\n",
    "    \n",
    "    # Check for duplicates, this adds a new column to the dataset\n",
    "    if printer: print(f'Count of duplicates: {len(df.duplicated())}')\n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    if printer: print(df.isnull().any())\n",
    "    \n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    if printer: print(f'See that there is only one column with NaN values: {df.isna().any()}')\n",
    "    if printer: print(f'\"HDI for year\" is the only column with NaN.  Let\\'s compare how many NaN {df[\"HDI for year\"].isna().count()} vs {df[\"HDI for year\"].notna().count()} non-NaN of a total of {len(df[\"HDI for year\"])} samples')\n",
    "    if printer: print(df[\"HDI for year\"].describe())\n",
    "    if printer: print(f'This is reporting specious results.  DataFrame.describe() is showing 8364 non-NaN values.  Let\\'s fill NaN with the means and see what that changes.')\n",
    "    \n",
    "    # Replace NaN values, or leave it as is otherwise\n",
    "    mean_value = df['HDI for year'].mean()\n",
    "    df['HDI for year'] = df['HDI for year'].fillna(mean_value)\n",
    "    # This shows that there are no longer any missing values\n",
    "    if printer: print(f'\\n\\nSee, no more missing values.{df.isna().any()}')\n",
    "    \n",
    "    \n",
    "    if printer: print('\\n\\n\\n')\n",
    "    ## Using the method described in the module notebook, check unique values by column\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            if printer: print(col, df[col].unique())\n",
    "    return df\n",
    "\n",
    "\n",
    "df4 = fix_data_before_numeric_or_ordinal_changes(df)\n",
    "\n",
    "# Scale the numeric features\n",
    "numeric_columns = list(df4.select_dtypes(exclude=['object']).columns)\n",
    "sc = StandardScaler()\n",
    "df4[numeric_columns] = sc.fit_transform(df4[numeric_columns])\n",
    "\n",
    "\n",
    "## LabelEncoder\n",
    "# Encode object types. They are all strings.  Save the labelencoders paired with the column names so we can reverse the values later\n",
    "columns_to_encode = df4.select_dtypes(include='object')\n",
    "encoders = dict.fromkeys(columns_to_encode)\n",
    "for column in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df4[column] = le.fit_transform(df4[column].astype(str))\n",
    "    encoders[column] = le\n",
    "\n",
    "\n",
    "# Use pandas to print correlation\n",
    "print('\\n\\n\\n')\n",
    "labels = list(df4.columns)\n",
    "feature_labels = list(df4.columns)\n",
    "target_label = 'suicides per 100k pop'\n",
    "feature_labels.remove(target_label)\n",
    "X = df4.drop(target_label, axis=1).values\n",
    "y = df4[target_label].values\n",
    "print(labels, '\\n',feature_labels, '\\n', target_label, '\\n', )\n",
    "print(f'\\n\\n{\"-\"*50}\\nPandas DataFrame.corrwith():\\n{\"-\"*50}\\n{df4.corrwith(df4[target_label]).sort_values()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987d691-2268-4e51-b85f-8a71c19362dc",
   "metadata": {},
   "source": [
    "### 5. [20 pts] Pre-process the dataset and list the major features you want to use. Note that not all features are crucial. For example, country-year variable is a derived feature and for a classifier it would not be necessary to include the year, the country and the country -year together. In fact, one must avoid adding a derived feature and the original at the same time.\n",
    "List the independent features you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fbabc-0f85-458d-9509-039e2e39edf7",
   "metadata": {},
   "source": [
    "#### 5. Answer\n",
    "My independent variables will be `generation`, `year`, `gdp_per_capita`, `gdp_for_year`, `HDI for year`, `country`, `sex`, and `age`.  I will not be using `year` or `generation` since they don't correlate with the target; and I will drop `suicides_no` and `population` since `suicides_no` /`population` = `suicides per 100k pop`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75406b89-4755-423e-a31c-cf92bbb7b3e5",
   "metadata": {},
   "source": [
    "### 6. [20 pts] Devise a classification problem and present a working prototype model. (It does not have to perform great, but it has to be functional.) Note that we will continue with this problem in the following modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1d1b6-b25a-4e98-b5d1-d88bdf6c50f2",
   "metadata": {},
   "source": [
    "#### 6. Answer\n",
    "\n",
    "Since I am calling this a regression problem I will use a regressor from Scikit-Learn, `GradientBoostingRegressor`.  The documentation on `GradientBoostingRegressor` uses the Diabetes dataset, which is totally composed of numeric features.  I am using the `max_depth` of 3 since it is close to the square root of the 8 features I've chosen.\n",
    "\n",
    "I tested the `GradientBoostingRegressor` with normalized and standardized features. Normalized features resulted in lower MAE and MSE.  I omitted printing the graphs, which could be done by setting the `plotter` flag to `True` in the second run.\n",
    "\n",
    "\n",
    "Recall that in `#4` I extracted `X` and `y` for correlation.  I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94df9c82-d164-493e-b55d-870f65956c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def answer_q_6(X, y, clf=None, plotter=True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, clf.predict(X_test))\n",
    "    print(\"The mean absoluge error (MAE) on test set: {:.10f}\".format(mae))\n",
    "    \n",
    "    mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "    print(\"The mean squared error (MSE) on test set: {:.10f}\".format(mse))\n",
    "    \n",
    "\n",
    "    if plotter:\n",
    "        test_score_mae = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "        for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "            test_score_mae[i] = mean_absolute_error(y_test, y_pred)\n",
    "            \n",
    "        test_score_mse = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "        for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "            test_score_mse[i] = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.title(\"Deviance MAE\")\n",
    "        plt.plot(\n",
    "            np.arange(params[\"n_estimators\"]) + 1,\n",
    "            clf.train_score_,\n",
    "            \"b-\",\n",
    "            label=\"Training Set Deviance\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(params[\"n_estimators\"]) + 1, test_score_mae, \"r-\", label=\"Test Set Deviance MAE\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Deviance MSE\")\n",
    "        plt.plot(\n",
    "            np.arange(params[\"n_estimators\"]) + 1,\n",
    "            clf.train_score_,\n",
    "            \"b-\",\n",
    "            label=\"Training Set Deviance\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(params[\"n_estimators\"]) + 1, test_score_mse, \"r-\", label=\"Test Set Deviance MSE\"\n",
    "        )\n",
    "        \n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xlabel(\"Boosting Iterations\")\n",
    "        plt.ylabel(\"Deviance\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "answer_q_6(X, y, clf=clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b87ca3-5510-41c6-9d0c-01b9715fdd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absoluge error (MAE) on test set: 0.0000000028\n",
      "The mean squared error (MSE) on test set: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "df6 = fix_data_before_numeric_or_ordinal_changes(df, printer=False)\n",
    "numeric_columns = list(df6.select_dtypes(exclude=['object']).columns)\n",
    "nm = Normalizer()\n",
    "df6[numeric_columns] = nm.fit_transform(df6[numeric_columns])\n",
    "\n",
    "\n",
    "## LabelEncoder\n",
    "# Encode object types. They are all strings.  Save the labelencoders paired with the column names so we can reverse the values later\n",
    "columns_to_encode = df6.select_dtypes(include='object')\n",
    "encoders = dict.fromkeys(columns_to_encode)\n",
    "for column in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df6[column] = le.fit_transform(df6[column].astype(str))\n",
    "    encoders[column] = le\n",
    "\n",
    "\n",
    "labels = list(df6.columns)\n",
    "feature_labels = list(df6.columns)\n",
    "target_label = 'suicides per 100k pop'\n",
    "feature_labels.remove(target_label)\n",
    "## Preserve X, y \n",
    "X6 = df6.drop(target_label, axis=1).values\n",
    "y6 = df6[target_label].values\n",
    "\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "answer_q_6(X6, y6, clf=clf, plotter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771cb1b-6b4a-40d1-87af-82740e585329",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Raschka, Sebastian, et al. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022.\n",
    "2. Guven, Erhan. Applied Machine Learning: Module 3 Notebook.  Last accessed 6 February, 2025.\n",
    "3. Scikit-Learn. https://scikit-learn.org/stable/. Last accessed 6 February, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac1b0f-2924-42c0-9667-190368b3c473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a27156-ff96-412d-a6f2-c42c40476329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ecc55-9789-49d9-8abe-2dd9d15e2124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888aa7e-89a3-4b39-9629-89cb9ad86148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27879e-8ce0-46fa-9da5-bcbab92e7bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb21f35-6744-4446-bc65-26f709f090d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b34b0-d585-48f8-8119-0a456b231aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b502d-34c7-4a99-b1bb-4894caca42c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe0840-bd5e-416e-8539-a2259591a392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01969016-0980-4966-9189-b8081cf9542c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ab1f5-0065-4eec-b3b1-a58707b5a585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9851803-8a10-42c4-9062-0b7fc713d033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93142238-f725-4f6e-82ef-8fc42c24a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e36f3-54ad-416c-a0d6-a4786223aefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cdfe40-739b-4e2f-ad50-25b04c60c8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501efe7-880e-4ee8-b6bb-c66dbf4bb487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61424ea1-4c5e-48ec-99c7-05b30c2ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# df_rfc = fix_data_before_numeric_or_ordinal_changes(df, printer=False)\n",
    "\n",
    "\n",
    "# numeric_columns = list(df_rfc.select_dtypes(exclude=['object']).columns)\n",
    "# sc = StandardScaler()\n",
    "# df_rfc[numeric_columns] = sc.fit_transform(df_rfc[numeric_columns])\n",
    "\n",
    "\n",
    "# # Use the one-hot encoder from Module 3 notebook\n",
    "# # pandas get_dummies function is the one-hot-encoder\n",
    "# def encode_onehot(_df, _f):\n",
    "#     _df2 = pd.get_dummies(_df[_f], prefix=_f, prefix_sep=' - ', dtype=int)\n",
    "#     _df3 = pd.concat([_df, _df2], axis=1)\n",
    "#     _df3 = _df3.drop([_f], axis=1)\n",
    "#     return _df3\n",
    "\n",
    "# columns_to_encode = list(df_rfc.select_dtypes(include='object').columns)\n",
    "# for column in columns_to_encode:\n",
    "#     df_rfc = encode_onehot(df_rfc, column)\n",
    "\n",
    "\n",
    "# labels = list(df_rfc.columns)\n",
    "# feature_labels = list(df_rfc.columns)\n",
    "# target_label = 'suicides per 100k pop'\n",
    "# feature_labels.remove(target_label)\n",
    "# Xrfc = df_rfc.drop(target_label, axis=1).values\n",
    "# yrfc = df_rfc[target_label].values\n",
    "\n",
    "# params = {\n",
    "#     \"n_estimators\": 500,\n",
    "#     \"criterion\": \"entropy\",\n",
    "#     \"max_depth\": 3,\n",
    "#     \"n_jobs\": 2,\n",
    "#     \"random_state\": 42,\n",
    "# }\n",
    "# clf = RandomForestClassifier(**params)\n",
    "# answer_q_6(Xrfc, yrfc, clf=clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
