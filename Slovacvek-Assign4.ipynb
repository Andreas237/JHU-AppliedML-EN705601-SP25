{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01920f40-cea1-4a52-92f8-cb49acd5b4e4",
   "metadata": {},
   "source": [
    "# Assignmnet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd174b-e513-4392-a5c7-073496f79323",
   "metadata": {},
   "source": [
    "Generally, a parameter selection procedure might be necessary to evaluate Probability of\n",
    "Detection versus Probability of False Alarm (i.e., Pd versus Pf) in order to select a classifier\n",
    "model and/or select a value for a hyperparameter for a classifier.\n",
    "                                                \n",
    "In this assignment we will produce an ROC plot presenting operating points of various\n",
    "classifiers and their varying hyperparameters so that we can make a justifiable operating\n",
    "classifier/parameter selection for the following problem.\n",
    "\n",
    "The classification of fake news or misinformation is a very important task today. Download the\n",
    "fake news dataset (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset),\n",
    "Fake.csv and True.csv files. Load the datasets into your model development framework and\n",
    "examine the features to confirm that they are text in title and text columns. Set fake as 1\n",
    "and true as 0. Concatenate the datasets together to produce one dataset of around 44,880\n",
    "rows. Apply necessary pre-processing to extract the title column with Tf-Idf. (This assigns\n",
    "numerical values to terms based on their frequency in a given document and throughout a\n",
    "given collection of documents.) Use around 50 features. Make sure to include a sanity check in\n",
    "the pipeline and perhaps run your favorite baseline classifier first.\n",
    "\n",
    "```\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "X = TfidfVectorizer(stop_words='english',\n",
    "max_features=40).fit_transform(df['title'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273a94f-4e7a-4e25-be95-9e66987e168f",
   "metadata": {},
   "source": [
    "## 1. [70 pts]\n",
    "\n",
    "By using three classifiers—decision tree, random forest, and neural network—and\n",
    "at least 2 different hyperparameter settings for each, generate operating points and plot\n",
    "them on a ROC. In particular, plot mean TPR and mean FPR, where the means are taken\n",
    "from the multiple runs of cross-validations. Do not hesitate to use/modify the ROC plot code\n",
    "in the module notebook if necessary. In case you do not see enough variety in Pd-Pf you\n",
    "might need to work on the classifiers set and/or hyperparameters. And do not hesitate to try\n",
    "hundreds, if necessary, since the ROC is just a natural scatter plot.\n",
    "(Some recommended parameters and ranges: depth [3-12], number of features [3-20],\n",
    "number of estimators [20-100], layer size [1-10], learning rate; and total of 10-20 Ops.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a90917-585f-4a38-8393-f5321c639a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets \n",
    "import pandas as pd\n",
    "\n",
    "df_true = pd.read_csv('datasets/True.csv')\n",
    "df_fake = pd.read_csv('datasets/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379fb637-43c2-43e8-932e-87e10b8da272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "True dataset head: \n",
      "                                                title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "\n",
      "\n",
      "\n",
      "Fake dataset head: \n",
      "                                                title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "The columns of the final dataset are: Index(['title', 'text', 'subject', 'date', 'class'], dtype='object')\n",
      "dataframe has 44898 samples.\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print('\\n\\n\\nTrue dataset head: \\n',df_true.head(n=2))\n",
    "print('\\n\\n\\nFake dataset head: \\n',df_fake.head(n=2))\n",
    "print(f'\\n\\n\\n{df_true.columns}')\n",
    "print(f'\\n\\n\\n{df_fake.columns}')\n",
    "\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "print(f'\\n\\n\\nThe columns of the final dataset are: {df.columns}')\n",
    "print(f'dataframe has {len(df)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e19b0-3f7e-4492-8483-2de2fc108252",
   "metadata": {},
   "source": [
    "### Split the data into training and testing, and try a \n",
    "\n",
    "Per the assignment prompt do a sanity-- in this case check that we have the correct number of samples, approximately 44,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a04e0db-ab2e-40ed-a44f-2646d44deaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 50) (44898,)\n"
     ]
    }
   ],
   "source": [
    "# Transform the titles into a vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_FEAUTRES = 50\n",
    "X = TfidfVectorizer(stop_words='english', max_features=MAX_FEAUTRES).fit_transform(df['title'])\n",
    "y = df['class']\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d193e40-0aab-49ed-8ff8-807efa0b437f",
   "metadata": {},
   "source": [
    "### Setup paramters across several classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe792d1-f80d-480e-8008-966f9749a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, HalvingRandomSearchCV, train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clfs = {\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=42),\n",
    "    'random_forest': RandomForestClassifier(random_state=42),\n",
    "    'neural_network': Perceptron(random_state=42)\n",
    "}\n",
    "param_dict = {\n",
    "    'halving_search': {\n",
    "        'decision_tree': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37],\n",
    "            'ccp_alpha': np.linspace(0.0, 5, 10),\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37],\n",
    "            'ccp_alpha': np.linspace(0.0, 5, 10),\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'random_state': [None, 42],\n",
    "            'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "            'eta0': np.linspace(.5,5,10),\n",
    "            'early_stopping': [True, False],\n",
    "        }\n",
    "    },\n",
    "    'grid_search': {\n",
    "        'decision_tree': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13],\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13],\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'random_state': [None, 42],\n",
    "            'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "            'eta0': np.linspace(.5,5,10),\n",
    "            'early_stopping': [True, False],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b259a-a194-4d83-95f5-9beb81c6d290",
   "metadata": {},
   "source": [
    "### Get the best hyperparameters for each model\n",
    "\n",
    "Note: after testing with GridSeach, and the amount of params I put in, the timespace was untenable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f787e6-128b-406d-8dfd-dbe41a4c2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "running halving random search for decision_tree\n",
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 20\n",
      "max_resources_: 35918\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1795\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 1795 candidates, totalling 8975 fits\n"
     ]
    }
   ],
   "source": [
    "bests = {}\n",
    "for k in clfs.keys():\n",
    "    print(f'\\n\\n\\nrunning halving random search for {k}')\n",
    "    hs = HalvingRandomSearchCV(clfs[k], param_dict['halving_search'][k], cv=5, scoring='accuracy', n_jobs=8, verbose=1)\n",
    "    hs.fit(X_train, y_train)\n",
    "    bests[k+'__HalvingSearch'] = { \n",
    "                'best_model': hs.best_estimator_,\n",
    "                'best_accuracy': hs.best_estimator_.score(X_test, y_test),\n",
    "                'best_params': hs.best_params_,\n",
    "                'best_score': hs.best_score_\n",
    "            }\n",
    "    # print(f'\\n\\n\\nrunning grid search for {k}')\n",
    "    # gs = GridSearchCV(clfs[k], param_dict['grid_search'][k], cv=5, scoring='accuracy', n_jobs=8, verbose=1)\n",
    "    # gs.fit(X_train, y_train)\n",
    "    # bests[k+'__GridSearch'] = { \n",
    "    #             'best_model': gs.best_estimator_,\n",
    "    #             'best_accuracy': gs.best_estimator_.score(X_test, y_test),\n",
    "    #             'best_params': gs.best_params_,\n",
    "    #             'best_score': gs.best_score_\n",
    "    #         }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7275e8f-94c8-4ba7-9ed7-7a18bf7e0931",
   "metadata": {},
   "source": [
    "### For my own sake print the best parts of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124adad4-f68c-4055-bc0d-7438dc751bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_clf, clf_scores in bests.items():\n",
    "    print(f'For {str(k_clf).replace('__HalvingSearch',' with halving random search').replace('__GridSearch',' with grid search')} achieved')\n",
    "    for k, v in clf_scores.items():\n",
    "        print(f'\\t\\t{k}: {v}')\n",
    "    print()\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f190bd1-f749-4012-a030-b7e6b83c605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(TPR, FPR):\n",
    "    fpr = [0.]+list(FPR)+[1.]; tpr = [0.]+list(TPR)+[1.]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(dpi=72)\n",
    "    plt.plot(fpr, tpr, ':', label='ROC')\n",
    "    plt.scatter(FPR, TPR, 50, color='red', marker='o', label='operating points')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='coin flip')\n",
    "    \n",
    "    # Annotate certain operating points\n",
    "    annot(1, fpr[1], tpr[1])\n",
    "    annot(2, fpr[4], tpr[4])\n",
    "    annot(3, fpr[8], tpr[8])\n",
    "    annot(4, fpr[9], tpr[9])\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2168bc-200e-4697-874c-4cd49c389681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, roc_curve\n",
    "\n",
    "roc_plots = []\n",
    "for k_clf, clf_scores in bests.items():\n",
    "    model_name = k_clf.replace('__HalvingSearch','').replace('__GridSearch','')\n",
    "    print(model_name)\n",
    "    y_pred = clf_scores['best_model'].predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]  # Pd\n",
    "    FPR += [fp/(fp+tn)]  # Pf\n",
    "    # plotter(TPR, FPR)\n",
    "    roc_plots.append(RocCurveDisplay(tpr=TPR, fpr=FPR).plot())\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, len(roc_plots), figsize=(12, 8))\n",
    "\n",
    "for i in range(len(roc_plots)):\n",
    "    roc_plots[i].plot(ax=axs[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80a68c-7ccd-42e6-ad9a-91e7fa00aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4429ec4-f4a5-4904-af5d-6c134e3a2cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
