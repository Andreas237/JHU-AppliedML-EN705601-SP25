{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01920f40-cea1-4a52-92f8-cb49acd5b4e4",
   "metadata": {},
   "source": [
    "# Assignmnet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd174b-e513-4392-a5c7-073496f79323",
   "metadata": {},
   "source": [
    "Generally, a parameter selection procedure might be necessary to evaluate Probability of\n",
    "Detection versus Probability of False Alarm (i.e., Pd versus Pf) in order to select a classifier\n",
    "model and/or select a value for a hyperparameter for a classifier.\n",
    "                                                \n",
    "In this assignment we will produce an ROC plot presenting operating points of various\n",
    "classifiers and their varying hyperparameters so that we can make a justifiable operating\n",
    "classifier/parameter selection for the following problem.\n",
    "\n",
    "The classification of fake news or misinformation is a very important task today. Download the\n",
    "fake news dataset (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset),\n",
    "Fake.csv and True.csv files. Load the datasets into your model development framework and\n",
    "examine the features to confirm that they are text in title and text columns. Set fake as 1\n",
    "and true as 0. Concatenate the datasets together to produce one dataset of around 44,880\n",
    "rows. Apply necessary pre-processing to extract the title column with Tf-Idf. (This assigns\n",
    "numerical values to terms based on their frequency in a given document and throughout a\n",
    "given collection of documents.) Use around 50 features. Make sure to include a sanity check in\n",
    "the pipeline and perhaps run your favorite baseline classifier first.\n",
    "\n",
    "```\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "X = TfidfVectorizer(stop_words='english',\n",
    "max_features=40).fit_transform(df['title'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273a94f-4e7a-4e25-be95-9e66987e168f",
   "metadata": {},
   "source": [
    "## 1. [70 pts]\n",
    "\n",
    "By using three classifiers—decision tree, random forest, and neural network—and\n",
    "at least 2 different hyperparameter settings for each, generate operating points and plot\n",
    "them on a ROC. In particular, plot mean TPR and mean FPR, where the means are taken\n",
    "from the multiple runs of cross-validations. Do not hesitate to use/modify the ROC plot code\n",
    "in the module notebook if necessary. In case you do not see enough variety in Pd-Pf you\n",
    "might need to work on the classifiers set and/or hyperparameters. And do not hesitate to try\n",
    "hundreds, if necessary, since the ROC is just a natural scatter plot.\n",
    "(Some recommended parameters and ranges: depth [3-12], number of features [3-20],\n",
    "number of estimators [20-100], layer size [1-10], learning rate; and total of 10-20 Ops.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a90917-585f-4a38-8393-f5321c639a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets \n",
    "import pandas as pd\n",
    "\n",
    "df_true = pd.read_csv('datasets/True.csv')\n",
    "df_fake = pd.read_csv('datasets/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379fb637-43c2-43e8-932e-87e10b8da272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "True dataset head: \n",
      "                                                title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "\n",
      "\n",
      "\n",
      "Fake dataset head: \n",
      "                                                title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "The columns of the final dataset are: Index(['title', 'text', 'subject', 'date', 'class'], dtype='object')\n",
      "dataframe has 44898 samples.\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print('\\n\\n\\nTrue dataset head: \\n',df_true.head(n=2))\n",
    "print('\\n\\n\\nFake dataset head: \\n',df_fake.head(n=2))\n",
    "print(f'\\n\\n\\n{df_true.columns}')\n",
    "print(f'\\n\\n\\n{df_fake.columns}')\n",
    "\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "print(f'\\n\\n\\nThe columns of the final dataset are: {df.columns}')\n",
    "print(f'dataframe has {len(df)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e19b0-3f7e-4492-8483-2de2fc108252",
   "metadata": {},
   "source": [
    "Per the assignment prompt, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a04e0db-ab2e-40ed-a44f-2646d44deaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 40)\n"
     ]
    }
   ],
   "source": [
    "# Transform the titles into a vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "X = TfidfVectorizer(stop_words='english', max_features=40).fit_transform(df['title'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d193e40-0aab-49ed-8ff8-807efa0b437f",
   "metadata": {},
   "source": [
    "### Test GridSearch optimal paramters across several classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe792d1-f80d-480e-8008-966f9749a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 31, 'max_features': None, 'splitter': 'best'}\n",
      "Best Accuracy: 0.814633203489928\n",
      "Test Accuracy: 0.8191536748329621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, HalvingRandomSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "MAX_FEAUTRES = 50\n",
    "\n",
    "\n",
    "X = TfidfVectorizer(stop_words='english', max_features=MAX_FEAUTRES).fit_transform(df['title'])\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "clfs = {\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "param_dict = {\n",
    "    'decision_tree': {\n",
    "                        'criterion':['gini', 'entropy', 'log_loss'],\n",
    "                        'max_features': ['sqrt', 'log2', None],\n",
    "                        'splitter': ['best', 'random'],\n",
    "                        'max_depth': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37],\n",
    "                        # 'ccp_alpha': np.linspace(0.0, 10.0, 100)\n",
    "                     }\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clfs['decision_tree'], param_dict['decision_tree'], cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f787e6-128b-406d-8dfd-dbe41a4c2bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m clfs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     gs \u001b[38;5;241m=\u001b[39m GridSearchCV(clfs[k], param_dict[k], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     bests[k] \u001b[38;5;241m=\u001b[39m { \n\u001b[0;32m----> 5\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m,\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m:    gs\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;241m.\u001b[39mscore(X_test, y_test),\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_params_,\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[1;32m      9\u001b[0m             }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "bests = {}\n",
    "for k in clfs.keys():\n",
    "    gs = GridSearchCV(clfs[k], param_dict[k], cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    bests[k] = { \n",
    "                'best_model': gs.best_estimator_,\n",
    "                'best_accuracy':    gs.best_estimator_.best_model.score(X_test, y_test),\n",
    "                'best_params': gs.best_params_,\n",
    "                'best_score': gs.best_score_\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2168bc-200e-4697-874c-4cd49c389681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
