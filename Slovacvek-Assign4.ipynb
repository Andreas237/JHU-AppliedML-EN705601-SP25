{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01920f40-cea1-4a52-92f8-cb49acd5b4e4",
   "metadata": {},
   "source": [
    "# Assignmnet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd174b-e513-4392-a5c7-073496f79323",
   "metadata": {},
   "source": [
    "Generally, a parameter selection procedure might be necessary to evaluate Probability of\n",
    "Detection versus Probability of False Alarm (i.e., Pd versus Pf) in order to select a classifier\n",
    "model and/or select a value for a hyperparameter for a classifier.\n",
    "                                                \n",
    "In this assignment we will produce an ROC plot presenting operating points of various\n",
    "classifiers and their varying hyperparameters so that we can make a justifiable operating\n",
    "classifier/parameter selection for the following problem.\n",
    "\n",
    "The classification of fake news or misinformation is a very important task today. Download the\n",
    "fake news dataset (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset),\n",
    "Fake.csv and True.csv files. Load the datasets into your model development framework and\n",
    "examine the features to confirm that they are text in title and text columns. Set fake as 1\n",
    "and true as 0. Concatenate the datasets together to produce one dataset of around 44,880\n",
    "rows. Apply necessary pre-processing to extract the title column with Tf-Idf. (This assigns\n",
    "numerical values to terms based on their frequency in a given document and throughout a\n",
    "given collection of documents.) Use around 50 features. Make sure to include a sanity check in\n",
    "the pipeline and perhaps run your favorite baseline classifier first.\n",
    "\n",
    "```\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "X = TfidfVectorizer(stop_words='english',\n",
    "max_features=40).fit_transform(df['title'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273a94f-4e7a-4e25-be95-9e66987e168f",
   "metadata": {},
   "source": [
    "## 1. [70 pts]\n",
    "\n",
    "By using three classifiers—decision tree, random forest, and neural network—and\n",
    "at least 2 different hyperparameter settings for each, generate operating points and plot\n",
    "them on a ROC. In particular, plot mean TPR and mean FPR, where the means are taken\n",
    "from the multiple runs of cross-validations. Do not hesitate to use/modify the ROC plot code\n",
    "in the module notebook if necessary. In case you do not see enough variety in Pd-Pf you\n",
    "might need to work on the classifiers set and/or hyperparameters. And do not hesitate to try\n",
    "hundreds, if necessary, since the ROC is just a natural scatter plot.\n",
    "(Some recommended parameters and ranges: depth [3-12], number of features [3-20],\n",
    "number of estimators [20-100], layer size [1-10], learning rate; and total of 10-20 Ops.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a90917-585f-4a38-8393-f5321c639a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets \n",
    "import pandas as pd\n",
    "\n",
    "df_true = pd.read_csv('datasets/True.csv')\n",
    "df_fake = pd.read_csv('datasets/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379fb637-43c2-43e8-932e-87e10b8da272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "True dataset head: \n",
      "                                                title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "\n",
      "\n",
      "\n",
      "Fake dataset head: \n",
      "                                                title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "The columns of the final dataset are: Index(['title', 'text', 'subject', 'date', 'class'], dtype='object')\n",
      "dataframe has 44898 samples.\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print('\\n\\n\\nTrue dataset head: \\n',df_true.head(n=2))\n",
    "print('\\n\\n\\nFake dataset head: \\n',df_fake.head(n=2))\n",
    "print(f'\\n\\n\\n{df_true.columns}')\n",
    "print(f'\\n\\n\\n{df_fake.columns}')\n",
    "\n",
    "df_true['class'] = 0; df_fake['class'] = 1\n",
    "df = pd.concat([df_fake, df_true])\n",
    "print(f'\\n\\n\\nThe columns of the final dataset are: {df.columns}')\n",
    "print(f'dataframe has {len(df)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e19b0-3f7e-4492-8483-2de2fc108252",
   "metadata": {},
   "source": [
    "### Split the data into training and testing, and try a \n",
    "\n",
    "Per the assignment prompt do a sanity-- in this case check that we have the correct number of samples, approximately 44,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a04e0db-ab2e-40ed-a44f-2646d44deaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 50) (44898,)\n"
     ]
    }
   ],
   "source": [
    "# Transform the titles into a vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_FEAUTRES = 50\n",
    "X = TfidfVectorizer(stop_words='english', max_features=MAX_FEAUTRES).fit_transform(df['title'])\n",
    "y = df['class']\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d193e40-0aab-49ed-8ff8-807efa0b437f",
   "metadata": {},
   "source": [
    "### Test GridSearch optimal paramters across several classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe792d1-f80d-480e-8008-966f9749a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, HalvingRandomSearchCV, train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clfs = {\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=42),\n",
    "    'random_forest': RandomForestClassifier(random_state=42),\n",
    "    'neural_network': Perceptron(random_state=42)\n",
    "}\n",
    "param_dict = {\n",
    "    'halving_search': {\n",
    "        'decision_tree': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37],\n",
    "            'ccp_alpha': np.linspace(0.0, 5, 10),\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37],\n",
    "            'ccp_alpha': np.linspace(0.0, 5, 10),\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'random_state': [None, 42],\n",
    "            'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "            'eta0': np.linspace(.5,5,10),\n",
    "            'early_stopping': [True, False],\n",
    "        }\n",
    "    },\n",
    "    'grid_search': {\n",
    "        'decision_tree': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13],\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_depth': [2, 3, 5, 7, 11, 13],\n",
    "            'random_state': [None, 42],\n",
    "            'min_samples_leaf': MAX_FEAUTRES*[.01, .05, .1, .15, .2],\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'random_state': [None, 42],\n",
    "            'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "            'eta0': np.linspace(.5,5,10),\n",
    "            'early_stopping': [True, False],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f787e6-128b-406d-8dfd-dbe41a4c2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "running grid search for decision_tree\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decision_tree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m clfs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mrunning grid search for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     gs \u001b[38;5;241m=\u001b[39m GridSearchCV(clfs[k], \u001b[43mparam_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m     gs\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      6\u001b[0m     bests[k\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__GridSearch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m { \n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_estimator_,\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mscore(X_test, y_test),\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_params_,\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: gs\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[1;32m     11\u001b[0m             }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decision_tree'"
     ]
    }
   ],
   "source": [
    "bests = {}\n",
    "for k in clfs.keys():\n",
    "    print(f'\\n\\n\\nrunning grid search for {k}')\n",
    "    gs = GridSearchCV(clfs[k], param_dict['grid_search'][k], cv=5, scoring='accuracy', n_jobs=8, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    bests[k+'__GridSearch'] = { \n",
    "                'best_model': gs.best_estimator_,\n",
    "                'best_accuracy': gs.best_estimator_.score(X_test, y_test),\n",
    "                'best_params': gs.best_params_,\n",
    "                'best_score': gs.best_score_\n",
    "            }\n",
    "    print(f'\\n\\n\\nrunning halving random search for {k}')\n",
    "    hs = HalvingRandomSearchCV(clfs[k], param_dict['halving_search'][k], cv=5, scoring='accuracy', n_jobs=8, verbose=1)\n",
    "    hs.fit(X_train, y_train)\n",
    "    bests[k+'__HalvingSearch'] = { \n",
    "                'best_model': hs.best_estimator_,\n",
    "                'best_accuracy': hs.best_estimator_.score(X_test, y_test),\n",
    "                'best_params': hs.best_params_,\n",
    "                'best_score': hs.best_score_\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124adad4-f68c-4055-bc0d-7438dc751bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_clf, clf_scores in bests.items():\n",
    "    print(f'For {str(k_clf).replace('__HalvingSearch',' with halving random search').replace('__GridSearch',' with grid search')} achieved')\n",
    "    for k, v in clf_scores.items():\n",
    "        print(f'\\t\\t{k}: {v}')\n",
    "    print()\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2168bc-200e-4697-874c-4cd49c389681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
